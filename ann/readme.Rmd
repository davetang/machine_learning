---
title: "Artificial Neural Networks"
output:
   md_document:
      variant: markdown
---

```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_bw())
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = "img/")
```

## Introduction

This notebook is adapted from [this tutorial](https://datascienceplus.com/fitting-neural-network-in-r/).

Install packages if missing and load.

```{r load_package, message=FALSE, warning=FALSE}
.libPaths('/packages')
my_packages <- c('MASS', 'neuralnet')

for (my_package in my_packages){
   if(!require(my_package, character.only = TRUE)){
      install.packages(my_package, '/packages')
      library(my_package, character.only = TRUE)
   }
}
```

## Housing values in Boston

The `Boston` data set from the `MASS` package contains the following features:

* `crim` - per capita crime rate by town.
* `zn` - proportion of residential land zoned for lots over 25,000 sq.ft.
* `indus` - proportion of non-retail business acres per town.
* `chas` - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
* `nox` - nitrogen oxides concentration (parts per 10 million).
* `rm` - average number of rooms per dwelling.
* `age` - proportion of owner-occupied units built prior to 1940.
* `dis` - weighted mean of distances to five Boston employment centres.
* `rad` - index of accessibility to radial highways.
* `tax` - full-value property-tax rate per $10,000.
* `ptratio` - pupil-teacher ratio by town.
* `black` - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
* `lstat` - lower status of the population (percent).
* `medv` - median value of owner-occupied homes in $1000s.

```{r boston_data}
str(Boston)
any(is.na(Boston))
```

### Multiple linear regression

Carry out multiple linear regression by regressing the median value onto all other features.

```{r glm}
set.seed(500)
index <- sample(1:nrow(Boston), round(0.75*nrow(Boston)))
train <- Boston[index,]
test <- Boston[-index,]
lm.fit <- glm(medv ~ ., data=train)
summary(lm.fit)
```

The number of rooms has the highest _t_-statistic.

```{r rm_vs_medv}
ggplot(Boston, aes(rm, medv)) +
  geom_point() +
  labs(x = "Average number of rooms per dwelling", y = "Median value in $1,000")
```

Predict prices and calculate the mean squared error (MSE).

```{r glm_predict}
pr.lm <- predict(lm.fit, test)
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)
MSE.lm
```

### Neural network

First we will carry out [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) using:

![](https://latex.codecogs.com/png.image?\large&space;\dpi{110}\bg{white}&space;x'&space;=&space;\frac{x&space;-&space;min(x)}{max(x)&space;-&space;min(x)})

Manually perform min-max and compare `scale` approach (just for fun).

```{r compare_min_max}
x <- 1:20
x_a <- (x - min(x)) / (max(x) - min(x))
x_b <- as.vector(scale(x, center = min(x), scale = max(x) - min(x)))
identical(x_a, x_b)
```

Carrying out scaling on Boston data set.

```{r boston_scaled}
maxs <- apply(Boston, 2, max)
mins <- apply(Boston, 2, min)

scaled <- as.data.frame(
  scale(Boston, center = mins, scale = maxs - mins)
)

train_scaled <- scaled[index,]
test_scaled <- scaled[-index,]
```

Manually create formula as `f` since `neuralnet` does not recognise R formulae.

```{r build_formula}
n <- names(train_scaled)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
```

Train neural network using two hidden layers with 5 and 3 neurons, respectively.

```{r train_boston, fig.width = 8, fig.height=6}
nn <- neuralnet(f, data = train_scaled, hidden=c(5,3), linear.output = TRUE)
plot(nn, rep = "best")
```

Predict (scaled) value.

```{r nn_predict}
pr.nn <- compute(nn, test_scaled[,1:13])
```

We need to unscale the data before calculating the MSE.

```{r mse_nn}
pr.nn_unscaled <- pr.nn$net.result * (max(Boston$medv) - min(Boston$medv)) + min(Boston$medv)
test.r <- (test_scaled$medv) * (max(Boston$medv) - min(Boston$medv)) + min(Boston$medv)

MSE.nn <- sum((test.r - pr.nn_unscaled)^2)/nrow(test_scaled)
```

Comparing the MSEs.

```{r mse_comp}
print(paste0("MSE of multiple linear regression: ", MSE.lm))
print(paste0("MSE of neural network regression: ", MSE.nn))
```

## Breast cancer data

Classify breast cancer samples using the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).

```{r breast_cancer_data}
data <- read.table(
   "../data/breast_cancer_data.csv",
   stringsAsFactors = FALSE,
   sep = ',',
   header = TRUE
)
data$class <- factor(data$class)
data <- data[,-1]
```

Separate into training (80%) and testing (20%).

```{r split_breast_cancer_data}
set.seed(31)
my_prob <- 0.8
my_split <- as.logical(
  rbinom(
    n = nrow(data),
    size = 1,
    p = my_prob
  )
)

train <- data[my_split,]
test <- data[!my_split,]
```

Train neural network.

```{r train_breast_cancer_data, fig.width = 8, fig.height=6}
n <- names(train)
f <- as.formula(paste("class ~", paste(n[!n %in% "class"], collapse = " + ")))
nn <- neuralnet(f, data = train, hidden=c(5,3), linear.output = FALSE)
plot(nn, rep = "best")
```

Predict and check results.

```{r predict_breast_cancer_data}
result <- compute(nn, test[,-10])
result <- apply(result$net.result, 1, function(x) ifelse(x[1] > x[2], yes = 2, no = 4))

# test$class are the rows and nn result are the columns
table(test$class, result)
```

## Further reading

The neuralnet [reference manual](https://cran.r-project.org/web/packages/neuralnet/neuralnet.pdf).

## Session info

Time built.

```{r time, echo=FALSE}
Sys.time()
```

Session info.

```{r session_info, echo=FALSE}
sessionInfo()
```
