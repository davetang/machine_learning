Introduction
------------

The `caret` [package](https://topepo.github.io/caret/) (Classification
And REgression Training) is a set of functions that attempt to
streamline the process for creating predictive models. The package
contains tools for, by is not limited to:

-   data splitting
-   pre-processing
-   feature selection
-   model tuning using resampling
-   variable importance estimation

This README was generated by running from the root directory of this
repository:

    script/rmd_to_md.sh template/template.Rmd

Packages
--------

Install packages if missing and load.

    .libPaths('/packages')
    my_packages <- c('gbm', 'mlbench', 'caret', 'xgboost', 'doParallel')

    for (my_package in my_packages){
      if(!require(my_package, character.only = TRUE)){
        install.packages(my_package, '/packages')
      }
      library(my_package, character.only = TRUE)
    }

Breast cancer data
------------------

Using the [Breast Cancer Wisconsin (Diagnostic) Data
Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).

    data <- read.table(
       "../data/breast_cancer_data.csv",
       stringsAsFactors = FALSE,
       sep = ',',
       header = TRUE
    )
    data$class <- factor(data$class)
    data <- data[,-1]

    str(data)

    ## 'data.frame':    599 obs. of  10 variables:
    ##  $ ct     : int  5 5 3 6 8 1 2 1 2 5 ...
    ##  $ ucsize : int  1 4 1 8 10 1 1 1 1 3 ...
    ##  $ ucshape: int  1 4 1 8 10 1 2 1 1 3 ...
    ##  $ ma     : int  1 5 1 1 8 1 1 1 1 3 ...
    ##  $ secs   : int  2 7 2 3 7 2 2 1 2 2 ...
    ##  $ bn     : int  1 10 2 4 10 10 1 1 1 3 ...
    ##  $ bc     : int  3 3 3 3 9 3 3 3 2 4 ...
    ##  $ nn     : int  1 2 1 7 7 1 1 1 1 4 ...
    ##  $ miti   : int  1 1 1 1 1 1 1 1 1 1 ...
    ##  $ class  : Factor w/ 2 levels "2","4": 1 1 1 1 2 1 1 1 1 2 ...

Pre-processing
--------------

### Zero- and near zero-variance predictors

Predictors that have zero- or near zero-variance are not useful for
making predictions since different classes will have the same values.

    data(mdrr)

    nzv <- nearZeroVar(mdrrDescr, saveMetrics = TRUE)
    nzv[nzv$nzv, ][1:6, ]

    ##      freqRatio percentUnique zeroVar  nzv
    ## nTB   23.00000     0.3787879   FALSE TRUE
    ## nBR  131.00000     0.3787879   FALSE TRUE
    ## nI   527.00000     0.3787879   FALSE TRUE
    ## nR03 527.00000     0.3787879   FALSE TRUE
    ## nR08 527.00000     0.3787879   FALSE TRUE
    ## nR11  21.78261     0.5681818   FALSE TRUE

Remove zero- and near zero-variance predictors.

    nzv <- nearZeroVar(mdrrDescr)
    filtered_descr <- mdrrDescr[, -nzv]
    dim(filtered_descr)

    ## [1] 528 297

### Correlated predictors

The `findCorrelation` function flags correlated predictors for removal.

    descr_cor <- cor(filtered_descr)
    summary(descr_cor[lower.tri(descr_cor)])

    ##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
    ## -0.99607 -0.05373  0.25006  0.26078  0.65527  1.00000

Remove highly correlated predictors.

    highly_cor_descr <- findCorrelation(descr_cor, cutoff = 0.75)
    filtered_descr <- filtered_descr[, -highly_cor_descr]

    descr_cor_post <- cor(filtered_descr)
    summary(descr_cor_post[lower.tri(descr_cor_post)])

    ##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
    ## -0.70728 -0.05378  0.04418  0.06692  0.18858  0.74458

Data dimension after removing correlated predictors.

    dim(filtered_descr)

    ## [1] 528  50

### Linear dependencies

Linear dependencies can occur when large numbers of binary chemical
fingerprints are used to describe the structure of a molecule. The
function `findLinearCombos` uses the QR decomposition of a matrix to
enumerate sets of linear combinations.

    ltfr_design <- matrix(0, nrow=6, ncol=6)
    ltfr_design[,1] <- c(1, 1, 1, 1, 1, 1)
    ltfr_design[,2] <- c(1, 1, 1, 0, 0, 0)
    ltfr_design[,3] <- c(0, 0, 0, 1, 1, 1)
    ltfr_design[,4] <- c(1, 0, 0, 1, 0, 0)
    ltfr_design[,5] <- c(0, 1, 0, 0, 1, 0)
    ltfr_design[,6] <- c(0, 0, 1, 0, 0, 1)

    colnames(ltfr_design) <- paste0("c", 1:6)
    rownames(ltfr_design) <- paste0("r", 1:6)
    ltfr_design

    ##    c1 c2 c3 c4 c5 c6
    ## r1  1  1  0  1  0  0
    ## r2  1  1  0  0  1  0
    ## r3  1  1  0  0  0  1
    ## r4  1  0  1  1  0  0
    ## r5  1  0  1  0  1  0
    ## r6  1  0  1  0  0  1

Note that columns two and three add up to the first column and columns
four, five, and six also add up to the first column. `findLinearCombos`
will return a list that enumerates these dependencies.

    combo_info <- findLinearCombos(ltfr_design)
    combo_info

    ## $linearCombos
    ## $linearCombos[[1]]
    ## [1] 3 1 2
    ## 
    ## $linearCombos[[2]]
    ## [1] 6 1 4 5
    ## 
    ## 
    ## $remove
    ## [1] 3 6

Remove linear dependencies.

    ltfr_design[, -combo_info$remove]

    ##    c1 c2 c4 c5
    ## r1  1  1  1  0
    ## r2  1  1  0  1
    ## r3  1  1  0  0
    ## r4  1  0  1  0
    ## r5  1  0  0  1
    ## r6  1  0  0  0

### The `preProcess` function

The `preProcess` class can be used for many operations on predictors,
including centering and scaling. The function estimates the required
parameters for each operation and `predict.preProcess` is used to apply
them to specific data sets.

#### Centring and scaling

The function `preProcess` does not pre-process the data but
`predict.preProcess` is used for pre-processing.

    set.seed(1984)
    train_idx <- sample(seq(along = mdrrClass), length(mdrrClass)/2)

    training <- filtered_descr[train_idx, ]
    training_class <- mdrrClass[train_idx]
    test <- filtered_descr[-train_idx, ]
    test_class <- mdrrClass[-train_idx]

    pre_proc_values <- preProcess(training, method = c("center", "scale"))

    training_pre_proc <- predict(pre_proc_values, training)
    test_pre_proc <- predict(pre_proc_values, test)

Before pre-processing.

    training[1:6, 1:6]

    ##             AMW   Mp   Ms nDB nAB nS
    ## LOPERAMIDE 6.88 0.66 2.26   1  18  0
    ## MA1499     7.19 0.67 2.03   1  18  0
    ## ANTAZOLINE 6.80 0.67 1.92   0  14  0
    ## AY9944     7.25 0.67 1.94   0  12  0
    ## FENDILINE  6.44 0.68 1.88   0  18  0
    ## S791100    7.19 0.67 2.05   1  18  0

After pre-processing.

    training_pre_proc[1:6, 1:6]

    ##                   AMW        Mp         Ms        nDB        nAB        nS
    ## LOPERAMIDE -0.1215808 0.2441047  0.6561522  0.3708254  1.2428816 -0.349808
    ## MA1499      0.3210425 0.6144704 -0.4108420  0.3708254  1.2428816 -0.349808
    ## ANTAZOLINE -0.2358062 0.6144704 -0.9211435 -0.8842759  0.2118659 -0.349808
    ## AY9944      0.4067116 0.6144704 -0.8283614 -0.8842759 -0.3036419 -0.349808
    ## FENDILINE  -0.7498203 0.9848361 -1.1067077 -0.8842759  1.2428816 -0.349808
    ## S791100     0.3210425 0.6144704 -0.3180599  0.3708254  1.2428816 -0.349808

#### Transforming predictors

Principal Component Analysis (PCA) can be used to transform data to a
smaller sub-space where the new variables are uncorrelated with one
another. The `preProcess` class can apply this transformation by
including “pca” in the `method` argument (this will also force scaling
of the predictors).

Independent Component Analysis (ICA) can also be used to find new
variables that are linear combinations of the original set such that the
components are independent.

The spatial sign transformation projects the data for a predictor to the
unit circle in *p* dimensions, where *p* is the number of predictors.
Essentially a vector of data is divided by its norm.

### Data splitting

Separate into training (80%) and testing (20%).

    set.seed(1984)
    my_prob <- 0.8
    my_split <- as.logical(
      rbinom(
        n = nrow(data),
        size = 1,
        p = my_prob
      )
    )

    train <- data[my_split,]
    test <- data[!my_split,]

    my_df <- rbind(
      prop.table(table(data$class)),
      prop.table(table(train$class)),
      prop.table(table(test$class))
    )

    rownames(my_df) <- c('orig', 'train', 'test')
    my_df

    ##               2         4
    ## orig  0.6243740 0.3756260
    ## train 0.6396588 0.3603412
    ## test  0.5692308 0.4307692

The function `createDataPartition` can be used to create balanced splits
of the data. If the `y` argument to this function is a factor, the
random sampling occurs within each class and should preserve the overall
class distribution of the data.

    set.seed(1984)
    train_idx <- createDataPartition(
      y = data$class,
      p = 0.8,
      list = FALSE,
      times = 1
    )

    train_caret <- data[train_idx, ]
    test_caret <- data[-train_idx, ]

    my_df <- rbind(
      prop.table(table(data$class)),
      prop.table(table(train_caret$class)),
      prop.table(table(test_caret$class))
    )

    rownames(my_df) <- c('orig', 'train', 'test')
    my_df

    ##               2         4
    ## orig  0.6243740 0.3756260
    ## train 0.6250000 0.3750000
    ## test  0.6218487 0.3781513

Model training and tuning
-------------------------

There are several functions that help to streamline the model building
and evaluation process. The `train` function can be used to:

-   estimate model performance from a training set
-   evaluate, using resampling, the effect of model tuning parameters on
    performance
-   choose the “optimal” model across these parameters

The first step in tuning the model is to choose a set of parameters to
evaluate. Once the model and tuning parameter values have been defined,
the type of resampling should be specified. *k*-fold cross-validation
(once or repeated), leave-one-out cross-validation and bootstrap
resampling methods can be used by `train`. After resampling, the process
produces a profile of performance measures that is available for finding
the tuning parameter values that should be used.

### An example

Using the `Sonar` data from the `mlbench` package.

    data("Sonar")
    str(Sonar[, 1:6])

    ## 'data.frame':    208 obs. of  6 variables:
    ##  $ V1: num  0.02 0.0453 0.0262 0.01 0.0762 0.0286 0.0317 0.0519 0.0223 0.0164 ...
    ##  $ V2: num  0.0371 0.0523 0.0582 0.0171 0.0666 0.0453 0.0956 0.0548 0.0375 0.0173 ...
    ##  $ V3: num  0.0428 0.0843 0.1099 0.0623 0.0481 ...
    ##  $ V4: num  0.0207 0.0689 0.1083 0.0205 0.0394 ...
    ##  $ V5: num  0.0954 0.1183 0.0974 0.0205 0.059 ...
    ##  $ V6: num  0.0986 0.2583 0.228 0.0368 0.0649 ...

Split.

    set.seed(1984)
    idx <- createDataPartition(Sonar$Class, p = 0.75, list = FALSE)
    training <- Sonar[idx, ]
    testing <- Sonar[-idx, ]

The function `trainControl` can be used to specify the type of
resampling and in the example below we repeat a 10-fold cross-validation
10 times.

    fit_control <- trainControl(
      method = "repeatedcv",
      number = 10,
      repeats = 10
    )

On a side note, seeds are controlled using `trainControl`:

> an optional set of integers that will be used to set the seed at each
> resampling iteration. This is useful when the models are run in
> parallel. A value of NA will stop the seed from being set within the
> worker processes while a value of NULL will set the seeds using a
> random set of integers. Alternatively, a list can be used. The list
> should have B+1 elements where B is the number of resamples, unless
> method is “boot632” in which case B is the number of resamples plus 1.
> The first B elements of the list should be vectors of integers of
> length M where M is the number of models being evaluated. The last
> element of the list only needs to be a single integer (for the final
> model). See the Examples section below and the Details section.

The first two arguments to `train` are the predictor and outcome data
objects, respectively, but a formula can be used instead as per the
example. The third argument, `method`, specifies the type of model. In
the example below, we fit a boosted tree model via the `gbm` package.

    set.seed(1984)
    system.time(
      gbm_fit <- train(
        Class ~ .,
        data = training,
        method = "gbm",
        trControl = fit_control,
        verbose = FALSE
      )
    )

    ##    user  system elapsed 
    ##  15.326   0.172  15.534

    gbm_fit

    ## Stochastic Gradient Boosting 
    ## 
    ## 157 samples
    ##  60 predictor
    ##   2 classes: 'M', 'R' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold, repeated 10 times) 
    ## Summary of sample sizes: 141, 141, 141, 142, 141, 142, ... 
    ## Resampling results across tuning parameters:
    ## 
    ##   interaction.depth  n.trees  Accuracy   Kappa    
    ##   1                   50      0.8040147  0.6037850
    ##   1                  100      0.8369510  0.6707514
    ##   1                  150      0.8357647  0.6685990
    ##   2                   50      0.8302475  0.6571250
    ##   2                  100      0.8439191  0.6843235
    ##   2                  150      0.8483039  0.6936068
    ##   3                   50      0.8286446  0.6537330
    ##   3                  100      0.8460245  0.6889748
    ##   3                  150      0.8556593  0.7083322
    ## 
    ## Tuning parameter 'shrinkage' was held constant at a value of 0.1
    ## 
    ## Tuning parameter 'n.minobsinnode' was held constant at a value of 10
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final values used for the model were n.trees = 150, interaction.depth =
    ##  3, shrinkage = 0.1 and n.minobsinnode = 10.

For a Gradient Boosting Machine (GBM) model, there are three main tuning
parameters:

-   complexity of the tree called `interaction.depth`
-   number of iterations, i.e. trees (called `n.trees`)
-   learning rate: how quickly the algorithm adapts, called `shrinkage`
-   the minimum number of training set samples in a node to commence
    splitting (`n.minobsinnode`)

The default values tested for this model are shown in the first two
columns; `shrinkage` and `n.minobsinnode` are not shown because the grid
set of candidate models all use a single value for these tuning
parameters.

The `Accuracy` column is the overall agreement rate averaged over
cross-validation iterations. The agreement standard deviation is also
calculated from the cross-validation results. The `Kappa` column is
Cohen’s (unweighted) Kappa statistic averaged across the resampling
results.

`train` can also automatically create a grid of tuning parameters for
some models. By default, if *p* is the number of tuning parameters, the
grid size is 3<sup>*p*</sup>.

The `train` function has an argument called `preProcess` that is used to
specify what pre-processing should be carried out. This argument takes a
character string of methods that would normally be passed to the
`method` argument of the `preProcess` function.

The tuning parameter grid can be specified using the `tuneGrid` argument
in the `train` function. Use `expand.grid` function to create a grid;
the grid `gbm_grid` specifies the combination of parameters to be
tested.

    gbm_grid <- expand.grid(
      interaction.depth = c(1, 5, 9),
      n.trees = (1:30)*50,
      shrinkage = c(0.1, 0.2),
      n.minobsinnode = c(10, 20)
    )

    head(gbm_grid)

    ##   interaction.depth n.trees shrinkage n.minobsinnode
    ## 1                 1      50       0.1             10
    ## 2                 5      50       0.1             10
    ## 3                 9      50       0.1             10
    ## 4                 1     100       0.1             10
    ## 5                 5     100       0.1             10
    ## 6                 9     100       0.1             10

Train using parameters specified in our grid and controlled by settings
defined in `fit_control`.

    set.seed(1984)
    system.time(
      gbm_fit_grid <- train(
        Class ~ .,
        data = training,
        method = "gbm",
        trControl = fit_control,
        verbose = FALSE,
        tuneGrid = gbm_grid
      )
    )

    ##    user  system elapsed 
    ##   2.846   6.526  46.759

If you want the seeds used to train the models, you can look in
`control$seeds`.

    head(gbm_fit_grid$control$seeds)

    ## [[1]]
    ##  [1] 218298  56533 881312 145135 459537 833604 460241 240166 930989 786634
    ## [11] 121794 534614
    ## 
    ## [[2]]
    ##  [1] 627925 549288 903067 224331 609713 277174 214810 221500 815624 435391
    ## [11] 338248 664404
    ## 
    ## [[3]]
    ##  [1] 463469  66460  19034 672587 640352 929533 356671  58624 977903  52449
    ## [11] 814965 784504
    ## 
    ## [[4]]
    ##  [1] 880060 416395 219513 660074 766757 194494 850992 836983 450960 305428
    ## [11] 743228 735207
    ## 
    ## [[5]]
    ##  [1]  33219 326494 411494 493594 385283 601946 974393  11562 548845 105522
    ## [11]  63992 658943
    ## 
    ## [[6]]
    ##  [1] 422069 179536 651436 780932 380568 673035 526853 457941 445672  44016
    ## [11] 711206 981833

    length(gbm_fit_grid$control$seeds)

    ## [1] 101

The
[code](https://github.com/topepo/caret/blob/master/pkg/caret/R/train.default.R#L612-L617)
that generates the seeds, samples integers from 1,000,000 and creates a
list of list of seeds. `control$seeds` is 101 in length because we
performed a 10x10 CV and there needs to be 10x10+1 set of seeds.

The reason for why there are 12 seeds per list entry is a bit more
complicated. Each model contains a `loop` function that can create
multiple submodel predictions from the same object. Even though we
generated a grid with 360 parameter combinations, `caret` saves time by
only training combination with the most trees and then creating
submodels from this model. Therefore, if we kept the `n.trees` parameter
constant at the highest number of trees, there are only 12 combinations
and therefore only 12 seeds are required for each resampling.

    gbm_fit_grid$modelInfo$loop(gbm_grid)$loop

    ##    shrinkage interaction.depth n.minobsinnode n.trees
    ## 1        0.1                 1             10    1500
    ## 2        0.1                 1             20    1500
    ## 3        0.1                 5             10    1500
    ## 4        0.1                 5             20    1500
    ## 5        0.1                 9             10    1500
    ## 6        0.1                 9             20    1500
    ## 7        0.2                 1             10    1500
    ## 8        0.2                 1             20    1500
    ## 9        0.2                 5             10    1500
    ## 10       0.2                 5             20    1500
    ## 11       0.2                 9             10    1500
    ## 12       0.2                 9             20    1500

`resample` contains the performance metrics as well as the `Resample`
name.

    head(gbm_fit_grid$resample)

    ##    Accuracy     Kappa     Resample
    ## 1 1.0000000 1.0000000 Fold01.Rep08
    ## 2 0.8666667 0.7321429 Fold01.Rep07
    ## 3 0.9333333 0.8672566 Fold08.Rep07
    ## 4 0.8750000 0.7500000 Fold07.Rep07
    ## 5 1.0000000 1.0000000 Fold09.Rep06
    ## 6 0.8666667 0.7321429 Fold04.Rep05

Plot grid results.

    ggplot(gbm_fit_grid)

![](img/line_plot_grid_res-1.png)

Heatmap.

    trellis.par.set(caretTheme())
    plot(gbm_fit_grid, plotType = "level")

![](img/heatmap_grid_res-1.png)

Using area under the ROC curve as a performance metric.

    fit_control_roc <- trainControl(
      method = "repeatedcv",
      number = 5,
      repeats = 1,
      classProbs = TRUE,
      summaryFunction = twoClassSummary
    )

    set.seed(1984)
    system.time(
      gbm_fit_roc <- train(
        Class ~ .,
        data = training,
        method = "gbm",
        trControl = fit_control_roc,
        verbose = FALSE,
        tuneGrid = gbm_grid,
        metric = "ROC"
      )
    )

    ##    user  system elapsed 
    ##   1.424   0.012   3.716

Best model according to area under the ROC.

    slice_max(.data = gbm_fit_roc$results, order_by = ROC, n = 1)

    ##   shrinkage interaction.depth n.minobsinnode n.trees       ROC      Sens
    ## 1       0.2                 9             10    1200 0.9352521 0.7992647
    ##        Spec      ROCSD    SensSD     SpecSD
    ## 1 0.9180952 0.05181693 0.1278757 0.05816545

The best parameters are stored in `bestTune`.

    gbm_fit_roc$bestTune

    ##     n.trees interaction.depth shrinkage n.minobsinnode
    ## 324    1200                 9       0.2             10

Density plot.

    trellis.par.set(caretTheme())
    densityplot(gbm_fit_roc, pch = '|')

![](img/roc_density-1.png)

XGBoost
-------

Prepare sonar data.

    data(Sonar, package = "mlbench")

    features <- Sonar[, -ncol(Sonar)]
    labels <- Sonar[, ncol(Sonar)]

    features_cor <- cor(features)
    cor_idx <- findCorrelation(features_cor, cutoff = 0.90)
    features <- features[, -cor_idx]

    set.seed(1984)
    idx <- createDataPartition(Sonar$Class, p = 0.80, list = FALSE)
    my_train <- features[idx, ]
    my_train$class <- labels[idx]
    my_test <- features[-idx, ]
    my_test$class <- labels[-idx]

    dim(my_train)

    ## [1] 167  58

The `trainControl` function has a `p` parameter for setting the training
percentage; we’ll set it to 0.80. We will also save resampled metrics
and predictions.

-   `returnData` - A logical for saving the data
-   `returnResamp` - A character string indicating how much of the
    resampled summary metrics should be saved. Values can be “final”,
    “all” or “none”
-   `savePredictions` - an indicator of how much of the hold-out
    predictions for each resample should be saved. Values can be either
    “all”, “final”, or “none”. A logical value can also be used that
    convert to “all” (for true) or “none” (for false). “final” saves the
    predictions for the optimal tuning parameters.

<!-- -->

    xgb_control <- trainControl(
      method = "repeatedcv",
      p = 0.8,
      returnData = TRUE,
      returnResamp = 'all',
      savePredictions = 'all',
      number = 5,
      repeats = 1,
      classProbs = TRUE,
      summaryFunction = twoClassSummary
    )

Parameter grid.

    xgb_grid <- expand.grid(
      nrounds = c(5, 10),
      max_depth = 6:7,
      eta = c(0.1, 0.5),
      gamma = 0,
      subsample = c(.5, 1),
      colsample_bytree = c(0.8),
      min_child_weight = 1
    )

    dim(xgb_grid)

    ## [1] 16  7

Train using `xgbTree`.

    set.seed(1984)
    system.time(
      my_xgbtree <- train(
        class ~ .,
        data = my_train,
        method = "xgbTree",
        trControl = xgb_control,
        metric = "ROC",
        tuneGrid = xgb_grid,
        nthread = 4,
        verbose = 0
      )
    )

    ##    user  system elapsed 
    ##   8.381   0.130   8.397

Results contained in `my_xgbtree`.

    names(my_xgbtree)

    ##  [1] "method"       "modelInfo"    "modelType"    "results"      "pred"        
    ##  [6] "bestTune"     "call"         "dots"         "metric"       "control"     
    ## [11] "finalModel"   "preProcess"   "trainingData" "ptype"        "resample"    
    ## [16] "resampledCM"  "perfNames"    "maximize"     "yLimits"      "times"       
    ## [21] "levels"       "terms"        "coefnames"    "xlevels"

Check whether 80% of the data was used for training in the
cross-validations.

-   `index` - a list with elements for each resampling iteration. Each
    list element is a vector of integers corresponding to the rows used
    for training at that iteration.
-   `indexOut` - a list (the same length as index) that dictates which
    data are held-out for each resample (as integers). If NULL, then the
    unique set of samples not contained in index is used.

<!-- -->

    sapply(my_xgbtree$control$index, length) / nrow(my_train)

    ## Fold1.Rep1 Fold2.Rep1 Fold3.Rep1 Fold4.Rep1 Fold5.Rep1 
    ##  0.7964072  0.8023952  0.8023952  0.7964072  0.8023952

Number of models that should be trained and tested.

    xgb_control$number * xgb_control$repeats * nrow(xgb_grid)

    ## [1] 80

All prediction results are saved in `pred`.

    head(my_xgbtree$pred)

    ##   pred obs rowIndex         M         R eta max_depth gamma colsample_bytree
    ## 1    M   R        1 0.5512618 0.4487382 0.1         6     0              0.8
    ## 2    M   R        7 0.5717898 0.4282102 0.1         6     0              0.8
    ## 3    R   R        9 0.2010230 0.7989770 0.1         6     0              0.8
    ## 4    R   R       11 0.3477746 0.6522254 0.1         6     0              0.8
    ## 5    R   R       13 0.3870940 0.6129060 0.1         6     0              0.8
    ## 6    R   R       14 0.4443059 0.5556941 0.1         6     0              0.8
    ##   min_child_weight subsample nrounds   Resample
    ## 1                1       0.5      10 Fold1.Rep1
    ## 2                1       0.5      10 Fold1.Rep1
    ## 3                1       0.5      10 Fold1.Rep1
    ## 4                1       0.5      10 Fold1.Rep1
    ## 5                1       0.5      10 Fold1.Rep1
    ## 6                1       0.5      10 Fold1.Rep1

Final model with the best performing parameters.

    my_xgbtree$finalModel$params

    ## $eta
    ## [1] 0.5
    ## 
    ## $max_depth
    ## [1] 6
    ## 
    ## $gamma
    ## [1] 0
    ## 
    ## $colsample_bytree
    ## [1] 0.8
    ## 
    ## $min_child_weight
    ## [1] 1
    ## 
    ## $subsample
    ## [1] 0.5
    ## 
    ## $objective
    ## [1] "binary:logistic"
    ## 
    ## $nthread
    ## [1] 4
    ## 
    ## $validate_parameters
    ## [1] TRUE

    my_xgbtree$finalModel$niter

    ## [1] 10

Predict using the final model on the test data.

    prop.table(table(my_test$class, predict(my_xgbtree, newdata = my_test)))

    ##    
    ##              M          R
    ##   M 0.48780488 0.04878049
    ##   R 0.14634146 0.31707317

Alternatively, we can use the best set of parameters from `bestTune` to
fit just a single model.

    fitControl <- trainControl(method = "none", classProbs = TRUE)

    set.seed(1984)
    my_xgbtree_best <- train(
      class ~ .,
      data = my_train,
      method = "xgbTree",
      trControl = fitControl,
      metric = "ROC",
      tuneGrid = my_xgbtree$bestTune,
      nthread = 4,
      verbose = 0
    )

    prop.table(table(my_test$class, predict(my_xgbtree_best, newdata = my_test)))

    ##    
    ##              M          R
    ##   M 0.46341463 0.07317073
    ##   R 0.24390244 0.21951220

Session info
------------

Time built.

    ## [1] "2022-12-23 07:08:27 UTC"

Session info.

    ## R version 4.2.1 (2022-06-23)
    ## Platform: x86_64-pc-linux-gnu (64-bit)
    ## Running under: Ubuntu 20.04.4 LTS
    ## 
    ## Matrix products: default
    ## BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
    ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3
    ## 
    ## locale:
    ##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
    ##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
    ##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
    ##  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
    ##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
    ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       
    ## 
    ## attached base packages:
    ## [1] parallel  stats     graphics  grDevices utils     datasets  methods  
    ## [8] base     
    ## 
    ## other attached packages:
    ##  [1] doParallel_1.0.17 iterators_1.0.14  foreach_1.5.2     xgboost_1.6.0.1  
    ##  [5] caret_6.0-92      lattice_0.20-45   mlbench_2.1-3     gbm_2.1.8.1      
    ##  [9] forcats_0.5.1     stringr_1.4.0     dplyr_1.0.9       purrr_0.3.4      
    ## [13] readr_2.1.2       tidyr_1.2.0       tibble_3.1.7      ggplot2_3.3.6    
    ## [17] tidyverse_1.3.1  
    ## 
    ## loaded via a namespace (and not attached):
    ##  [1] nlme_3.1-157         fs_1.5.2             lubridate_1.8.0     
    ##  [4] httr_1.4.3           tools_4.2.1          backports_1.4.1     
    ##  [7] utf8_1.2.2           R6_2.5.1             rpart_4.1.16        
    ## [10] DBI_1.1.3            colorspace_2.0-3     nnet_7.3-17         
    ## [13] withr_2.5.0          tidyselect_1.1.2     compiler_4.2.1      
    ## [16] cli_3.3.0            rvest_1.0.2          xml2_1.3.3          
    ## [19] labeling_0.4.2       scales_1.2.0         proxy_0.4-27        
    ## [22] digest_0.6.29        rmarkdown_2.14       pkgconfig_2.0.3     
    ## [25] htmltools_0.5.2      parallelly_1.32.0    highr_0.9           
    ## [28] dbplyr_2.2.1         fastmap_1.1.0        rlang_1.0.3         
    ## [31] readxl_1.4.0         rstudioapi_0.13      farver_2.1.1        
    ## [34] generics_0.1.3       jsonlite_1.8.0       ModelMetrics_1.2.2.2
    ## [37] magrittr_2.0.3       Matrix_1.4-1         Rcpp_1.0.8.3        
    ## [40] munsell_0.5.0        fansi_1.0.3          lifecycle_1.0.1     
    ## [43] pROC_1.18.0          stringi_1.7.6        yaml_2.3.5          
    ## [46] MASS_7.3-57          plyr_1.8.7           recipes_1.0.1       
    ## [49] grid_4.2.1           listenv_0.8.0        crayon_1.5.1        
    ## [52] haven_2.5.0          splines_4.2.1        hms_1.1.1           
    ## [55] knitr_1.39           pillar_1.7.0         stats4_4.2.1        
    ## [58] future.apply_1.9.0   reshape2_1.4.4       codetools_0.2-18    
    ## [61] reprex_2.0.1         glue_1.6.2           evaluate_0.15       
    ## [64] data.table_1.14.2    modelr_0.1.8         vctrs_0.4.1         
    ## [67] tzdb_0.3.0           cellranger_1.1.0     gtable_0.3.0        
    ## [70] future_1.26.1        assertthat_0.2.1     xfun_0.31           
    ## [73] gower_1.0.0          prodlim_2019.11.13   broom_1.0.0         
    ## [76] e1071_1.7-11         class_7.3-20         survival_3.3-1      
    ## [79] timeDate_3043.102    hardhat_1.2.0        lava_1.6.10         
    ## [82] globals_0.15.1       ellipsis_0.3.2       ipred_0.9-13
