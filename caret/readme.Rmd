---
title: "Classification And REgression Training"
output: md_document
---

```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_bw())
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = "img/")
```

## Introduction

The `caret` [package](https://topepo.github.io/caret/) (Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for, by is not limited to:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

This README was generated by running from the root directory of this repository:

    script/rmd_to_md.sh template/template.Rmd

## Packages

Install packages if missing and load.

```{r load_package, message=FALSE, warning=FALSE}
.libPaths('/packages')
my_packages <- c('gbm', 'mlbench', 'caret', 'xgboost', 'doParallel')

for (my_package in my_packages){
  if(!require(my_package, character.only = TRUE)){
    install.packages(my_package, '/packages')
  }
  library(my_package, character.only = TRUE)
}
```

## Breast cancer data

Using the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).

```{r prepare_data}
data <- read.table(
   "../data/breast_cancer_data.csv",
   stringsAsFactors = FALSE,
   sep = ',',
   header = TRUE
)
data$class <- factor(data$class)
data <- data[,-1]

str(data)
```

## Pre-processing

### Zero- and near zero-variance predictors

Predictors that have zero- or near zero-variance are not useful for making predictions since different classes will have the same values.

```{r nzv}
data(mdrr)

nzv <- nearZeroVar(mdrrDescr, saveMetrics = TRUE)
nzv[nzv$nzv, ][1:6, ]
```

Remove zero- and near zero-variance predictors.

```{r remove_nzv}
nzv <- nearZeroVar(mdrrDescr)
filtered_descr <- mdrrDescr[, -nzv]
dim(filtered_descr)
```

### Correlated predictors

The `findCorrelation` function flags correlated predictors for removal.

```{r descr_cor}
descr_cor <- cor(filtered_descr)
summary(descr_cor[lower.tri(descr_cor)])
```

Remove highly correlated predictors.

```{r remove_cor}
highly_cor_descr <- findCorrelation(descr_cor, cutoff = 0.75)
filtered_descr <- filtered_descr[, -highly_cor_descr]

descr_cor_post <- cor(filtered_descr)
summary(descr_cor_post[lower.tri(descr_cor_post)])
```

Data dimension after removing correlated predictors.

```{r filtered_descr_no_cor}
dim(filtered_descr)
```

### Linear dependencies

Linear dependencies can occur when large numbers of binary chemical fingerprints are used to describe the structure of a molecule. The function `findLinearCombos` uses the QR decomposition of a matrix to enumerate sets of linear combinations.

```{r ltfr_design}
ltfr_design <- matrix(0, nrow=6, ncol=6)
ltfr_design[,1] <- c(1, 1, 1, 1, 1, 1)
ltfr_design[,2] <- c(1, 1, 1, 0, 0, 0)
ltfr_design[,3] <- c(0, 0, 0, 1, 1, 1)
ltfr_design[,4] <- c(1, 0, 0, 1, 0, 0)
ltfr_design[,5] <- c(0, 1, 0, 0, 1, 0)
ltfr_design[,6] <- c(0, 0, 1, 0, 0, 1)

colnames(ltfr_design) <- paste0("c", 1:6)
rownames(ltfr_design) <- paste0("r", 1:6)
ltfr_design
```

Note that columns two and three add up to the first column and columns four, five, and six also add up to the first column. `findLinearCombos` will return a list that enumerates these dependencies.

```{r combo_info}
combo_info <- findLinearCombos(ltfr_design)
combo_info
```

Remove linear dependencies.

```{r remove_linear}
ltfr_design[, -combo_info$remove]
```

### The `preProcess` function

The `preProcess` class can be used for many operations on predictors, including centering and scaling. The function estimates the required parameters for each operation and `predict.preProcess` is used to apply them to specific data sets.

#### Centring and scaling

The function `preProcess` does not pre-process the data but `predict.preProcess` is used for pre-processing.

```{r centre_and_scale}
set.seed(1984)
train_idx <- sample(seq(along = mdrrClass), length(mdrrClass)/2)

training <- filtered_descr[train_idx, ]
training_class <- mdrrClass[train_idx]
test <- filtered_descr[-train_idx, ]
test_class <- mdrrClass[-train_idx]

pre_proc_values <- preProcess(training, method = c("center", "scale"))

training_pre_proc <- predict(pre_proc_values, training)
test_pre_proc <- predict(pre_proc_values, test)
```

Before pre-processing.

```{r before_pre_process}
training[1:6, 1:6]
```

After pre-processing.

```{r after_pre_process}
training_pre_proc[1:6, 1:6]
```

#### Transforming predictors

Principal Component Analysis (PCA) can be used to transform data to a smaller sub-space where the new variables are uncorrelated with one another. The `preProcess` class can apply this transformation by including "pca" in the `method` argument (this will also force scaling of the predictors).

Independent Component Analysis (ICA) can also be used to find new variables that are linear combinations of the original set such that the components are independent.

The spatial sign transformation projects the data for a predictor to the unit circle in $p$ dimensions, where $p$ is the number of predictors. Essentially a vector of data is divided by its norm.

### Data splitting

Separate into training (80%) and testing (20%).

```{r split_data_manual}
set.seed(1984)
my_prob <- 0.8
my_split <- as.logical(
  rbinom(
    n = nrow(data),
    size = 1,
    p = my_prob
  )
)

train <- data[my_split,]
test <- data[!my_split,]

my_df <- rbind(
  prop.table(table(data$class)),
  prop.table(table(train$class)),
  prop.table(table(test$class))
)

rownames(my_df) <- c('orig', 'train', 'test')
my_df
```

The function `createDataPartition` can be used to create balanced splits of the data. If the `y` argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data.

```{r create_data_part}
set.seed(1984)
train_idx <- createDataPartition(
  y = data$class,
  p = 0.8,
  list = FALSE,
  times = 1
)

train_caret <- data[train_idx, ]
test_caret <- data[-train_idx, ]

my_df <- rbind(
  prop.table(table(data$class)),
  prop.table(table(train_caret$class)),
  prop.table(table(test_caret$class))
)

rownames(my_df) <- c('orig', 'train', 'test')
my_df
```

## Model training and tuning

There are several functions that help to streamline the model building and evaluation process. The `train` function can be used to:

* estimate model performance from a training set
* evaluate, using resampling, the effect of model tuning parameters on performance
* choose the "optimal" model across these parameters

The first step in tuning the model is to choose a set of parameters to evaluate. Once the model and tuning parameter values have been defined, the type of resampling should be specified. _k_-fold cross-validation (once or repeated), leave-one-out cross-validation and bootstrap resampling methods can be used by `train`. After resampling, the process produces a profile of performance measures that is available for finding the tuning parameter values that should be used.

### An example

Using the `Sonar` data from the `mlbench` package.

```{r sonar_data}
data("Sonar")
str(Sonar[, 1:6])
```

Split.

```{r sonar_split}
set.seed(1984)
idx <- createDataPartition(Sonar$Class, p = 0.75, list = FALSE)
training <- Sonar[idx, ]
testing <- Sonar[-idx, ]
```

The function `trainControl` can be used to specify the type of resampling and in the example below we repeat a 10-fold cross-validation 10 times.

```{r sonar_train_control}
fit_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)
```

On a side note, seeds are controlled using `trainControl`:

>an optional set of integers that will be used to set the seed at each resampling iteration. This is useful when the models are run in parallel. A value of NA will stop the seed from being set within the worker processes while a value of NULL will set the seeds using a random set of integers. Alternatively, a list can be used. The list should have B+1 elements where B is the number of resamples, unless method is "boot632" in which case B is the number of resamples plus 1. The first B elements of the list should be vectors of integers of length M where M is the number of models being evaluated. The last element of the list only needs to be a single integer (for the final model). See the Examples section below and the Details section.

The first two arguments to `train` are the predictor and outcome data objects, respectively, but a formula can be used instead as per the example. The third argument, `method`, specifies the type of model. In the example below, we fit a boosted tree model via the `gbm` package.

```{r sonar_boosted_tree}
set.seed(1984)
system.time(
  gbm_fit <- train(
    Class ~ .,
    data = training,
    method = "gbm",
    trControl = fit_control,
    verbose = FALSE
  )
)

gbm_fit
```

For a Gradient Boosting Machine (GBM) model, there are three main tuning parameters:

* complexity of the tree called `interaction.depth`
* number of iterations, i.e. trees (called `n.trees`)
* learning rate: how quickly the algorithm adapts, called `shrinkage`
* the minimum number of training set samples in a node to commence splitting (`n.minobsinnode`)

The default values tested for this model are shown in the first two columns; `shrinkage` and `n.minobsinnode` are not shown because the grid set of candidate models all use a single value for these tuning parameters.

The `Accuracy` column is the overall agreement rate averaged over cross-validation iterations. The agreement standard deviation is also calculated from the cross-validation results. The `Kappa` column is Cohen's (unweighted) Kappa statistic averaged across the resampling results.

`train` can also automatically create a grid of tuning parameters for some models. By default, if $p$ is the number of tuning parameters, the grid size is $3^p$.

The `train` function has an argument called `preProcess` that is used to specify what pre-processing should be carried out. This argument takes a character string of methods that would normally be passed to the `method` argument of the `preProcess` function.

The tuning parameter grid can be specified using the `tuneGrid` argument in the `train` function. Use `expand.grid` function to create a grid; the grid `gbm_grid` specifies the combination of parameters to be tested.

```{r gbm_grid}
gbm_grid <- expand.grid(
  interaction.depth = c(1, 5, 9),
  n.trees = (1:30)*50,
  shrinkage = c(0.1, 0.2),
  n.minobsinnode = c(10, 20)
)

head(gbm_grid)
```

Train using parameters specified in our grid and controlled by settings defined in `fit_control`.

```{r make_socket, include=FALSE}
cl <- makePSOCKcluster(16)
registerDoParallel(cl)
clusterEvalQ(cl, .libPaths("/packages"))
```

```{r train_with_grid}
set.seed(1984)
system.time(
  gbm_fit_grid <- train(
    Class ~ .,
    data = training,
    method = "gbm",
    trControl = fit_control,
    verbose = FALSE,
    tuneGrid = gbm_grid
  )
)
```

If you want the seeds used to train the models, you can look in `control$seeds`.

```{r control_seeds}
head(gbm_fit_grid$control$seeds)
length(gbm_fit_grid$control$seeds)
```

The [code](https://github.com/topepo/caret/blob/master/pkg/caret/R/train.default.R#L612-L617) that generates the seeds, samples integers from 1,000,000 and creates a list of list of seeds. `control$seeds` is 101 in length because we performed a 10x10 CV and there needs to be 10x10+1 set of seeds.

The reason for why there are 12 seeds per list entry is a bit more complicated. Each model contains a `loop` function that can create multiple submodel predictions from the same object. Even though we generated a grid with 360 parameter combinations, `caret` saves time by only training combination with the most trees and then creating submodels from this model. Therefore, if we kept the `n.trees` parameter constant at the highest number of trees, there are only 12 combinations and therefore only 12 seeds are required for each resampling.

```{r loop}
gbm_fit_grid$modelInfo$loop(gbm_grid)$loop
```

`resample` contains the performance metrics as well as the `Resample` name.

```{r resample}
head(gbm_fit_grid$resample)
```

Plot grid results.

```{r line_plot_grid_res}
ggplot(gbm_fit_grid)
```

Heatmap.

```{r heatmap_grid_res}
trellis.par.set(caretTheme())
plot(gbm_fit_grid, plotType = "level")
```

Using area under the ROC curve as a performance metric.

```{r auroc}
fit_control_roc <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 1,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

set.seed(1984)
system.time(
  gbm_fit_roc <- train(
    Class ~ .,
    data = training,
    method = "gbm",
    trControl = fit_control_roc,
    verbose = FALSE,
    tuneGrid = gbm_grid,
    metric = "ROC"
  )
)
```

Best model according to area under the ROC.

```{r slice_max}
slice_max(.data = gbm_fit_roc$results, order_by = ROC, n = 1)
```

The best parameters are stored in `bestTune`.

```{r best_tune}
gbm_fit_roc$bestTune
```

Density plot.

```{r roc_density}
trellis.par.set(caretTheme())
densityplot(gbm_fit_roc, pch = '|')
```

## XGBoost

Prepare sonar data.

```{r prep_sonar_data}
data(Sonar, package = "mlbench")

features <- Sonar[, -ncol(Sonar)]
labels <- Sonar[, ncol(Sonar)]

features_cor <- cor(features)
cor_idx <- findCorrelation(features_cor, cutoff = 0.90)
features <- features[, -cor_idx]

set.seed(1984)
idx <- createDataPartition(Sonar$Class, p = 0.80, list = FALSE)
my_train <- features[idx, ]
my_train$class <- labels[idx]
my_test <- features[-idx, ]
my_test$class <- labels[-idx]

dim(my_train)
```

The `trainControl` function has a `p` parameter for setting the training percentage; we'll set it to 0.80. We will also save resampled metrics and predictions.

* `returnData` - A logical for saving the data
* `returnResamp` - A character string indicating how much of the resampled summary metrics should be saved. Values can be "final", "all" or "none"
* `savePredictions` - an indicator of how much of the hold-out predictions for each resample should be saved. Values can be either "all", "final", or "none". A logical value can also be used that convert to "all" (for true) or "none" (for false). "final" saves the predictions for the optimal tuning parameters.

```{r xgb_control}
xgb_control <- trainControl(
  method = "repeatedcv",
  p = 0.8,
  returnData = TRUE,
  returnResamp = 'all',
  savePredictions = 'all',
  number = 5,
  repeats = 1,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
```

Parameter grid.

```{r xgb_grid}
xgb_grid <- expand.grid(
  nrounds = c(5, 10),
  max_depth = 6:7,
  eta = c(0.1, 0.5),
  gamma = 0,
  subsample = c(.5, 1),
  colsample_bytree = c(0.8),
  min_child_weight = 1
)

dim(xgb_grid)
```

Train using `xgbTree`.

```{r xgb_train_control, message=FALSE, warning=FALSE}
set.seed(1984)
system.time(
  my_xgbtree <- train(
    class ~ .,
    data = my_train,
    method = "xgbTree",
    trControl = xgb_control,
    metric = "ROC",
    tuneGrid = xgb_grid,
    nthread = 4,
    verbose = 0
  )
)
```

Results contained in `my_xgbtree`.

```{r xgbtree_results}
names(my_xgbtree)
```

`xgbTree` uses the same trick as `gbm` (since both methods are boosting methods) and creates sub-models from a model trained with the highest number of rounds.

```{r xgbtree_loop}
my_xgbtree$modelInfo$loop(xgb_grid)$loop
```

Check whether 80% of the data was used for training in the cross-validations.

* `index` - a list with elements for each resampling iteration. Each list element is a vector of integers corresponding to the rows used for training at that iteration.
* `indexOut` - a list (the same length as index) that dictates which data are held-out for each resample (as integers). If NULL, then the unique set of samples not contained in index is used.

```{r sample_index}
sapply(my_xgbtree$control$index, length) / nrow(my_train)
```

Number of models that should be trained and tested.

```{r xgb_num_models}
xgb_control$number * xgb_control$repeats * nrow(xgb_grid)
```

All prediction results are saved in `pred`.

```{r my_xgbtree_pred}
head(my_xgbtree$pred)
```

Final model with the best performing parameters.

```{r final_model_param}
my_xgbtree$finalModel$params
my_xgbtree$finalModel$niter
```

Predict using the final model on the test data.

```{r final_model_predict}
prop.table(table(my_test$class, predict(my_xgbtree, newdata = my_test)))
```

Alternatively, we can use the best set of parameters from `bestTune` to fit just a single model.

```{r manual_best_tune}
fitControl <- trainControl(method = "none", classProbs = TRUE)

set.seed(1984)
my_xgbtree_best <- train(
  class ~ .,
  data = my_train,
  method = "xgbTree",
  trControl = fitControl,
  metric = "ROC",
  tuneGrid = my_xgbtree$bestTune,
  nthread = 4,
  verbose = 0
)

prop.table(table(my_test$class, predict(my_xgbtree_best, newdata = my_test)))
```

```{r stop_cluster, include=FALSE}
stopCluster(cl)
```

## Session info

Time built.

```{r time, echo=FALSE}
Sys.time()
```

Session info.

```{r session_info, echo=FALSE}
sessionInfo()
```

