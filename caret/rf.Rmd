---
title: "Random Forest tuning"
output: md_document
---

```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_bw())
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = "img/")
```

## Introduction

This README was generated by running this notebook in an RStudio Server instance.

Install packages if missing and load.

```{r load_package, message=FALSE, warning=FALSE}
.libPaths('/packages')
my_packages <- c('ROCR', 'doParallel', 'e1071', 'randomForest', 'mlbench', 'caret')

for (my_package in my_packages){
  if(!require(my_package, character.only = TRUE)){
    install.packages(my_package, '/packages')
  }
  library(my_package, character.only = TRUE)
}
```

## Sonar data

From `?Sonar`

> This is the data set used by Gorman and Sejnowski in their study of the classification of sonar signals using a neural network. The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.
>
> Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.
>
> The label associated with each record contains the letter "R" if the object is a rock and "M" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.

```{r prepare_data}
data(Sonar, package = "mlbench")

my_feat <- Sonar[, -ncol(Sonar)]
my_lab <- Sonar[, ncol(Sonar)]

dim(my_feat)
```

## Pre-processing

### Zero- and near zero-variance predictors

Predictors that have zero- or near zero-variance are not useful for making predictions since different classes will have the same values.

```{r nzv}
nzv <- nearZeroVar(my_feat, saveMetrics = TRUE)
table(nzv$nzv)
```

### Correlated predictors

The `findCorrelation` function flags correlated predictors for removal.

```{r descr_cor}
my_cor <- cor(my_feat)
summary(my_cor[lower.tri(my_cor)])
```

Remove highly correlated predictors.

```{r remove_cor}
my_cor_feat <- findCorrelation(my_cor, cutoff = 0.90)

my_feat_filt <- my_feat[, -my_cor_feat]
my_cor <- cor(my_feat_filt)
summary(my_cor[lower.tri(my_cor)])
```

Data dimension after removing correlated predictors.

```{r filtered_descr_no_cor}
dim(my_feat_filt)
```

### Data splitting

Separate into training (80%) and testing (20%) using `createDataPartition`. If the `y` argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data.

```{r create_data_part}
set.seed(1984)
my_idx <- createDataPartition(
  y = my_lab,
  p = 0.8,
  list = FALSE,
  times = 1
)

my_train <- my_feat_filt[my_idx, ]
my_train$class <- my_lab[my_idx]
my_test <- my_feat_filt[-my_idx, ]
my_test$class <- my_lab[-my_idx]
```

## Model training and tuning

The function `trainControl` can be used to specify the type of resampling and in the example below we perform 10 by 10 cross-validation. `classProbs` (logical) refers to whether class probabilities should be computed for classification models (along with predicted values) in each resample. `twoClassSummary` computes sensitivity, specificity and the area under the ROC curve.

```{r}
?trainControl
```

* `method` - the resampling method
* `number` - either the number of folds or number of resampling iterations
* `repeats` - for repeated k-fold cross-validation only
* `p` - for leave-group out cross-validation: the training percentage
* `search` = grid or random, describing how the tuning parameter grid is determined
* `savePredictions` - an indicator of how much of the hold-out predictions for each resample should be saved
* `classProbs` - should class probabilities be computed for classification models (along with predicted values) in each resample
* `summaryFunction` - a function to compute performance metrics across resamples
* `sampling` - a single character value describing the type of additional sampling that is conducted after resampling (usually to resolve class imbalances)
* `

```{r fit_control}
fit_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10,
  p = 0.8,
  savePredictions = "final",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
```

Try all possible `mtry` values.

```{r gbm_grid}
gbm_grid <- expand.grid(
  mtry = 1:ncol(my_train)
)
```

[Train](https://topepo.github.io/caret/model-training-and-tuning.html) in parallel using half of all available CPUs. The following apporach is carried out:

```
1  Define sets of model parameter values to evaluate
2  for each parameter set do
3    for each resampling iteration do
4      Hold-out specific samples
5      [Optional] Pre-process the data
6      Fit the model on the remainder
7      Predict the hold-out samples
8    end
9    Calculate the average performance across hold-out predictions
10 end
11 Determine the optimal parameter set
12 Fit the final model to all the training data using the optimal parameter set
```

```{r train_my_rf}
ncore <- ceiling(detectCores() / 2)

cl <- makePSOCKcluster(ncore)
registerDoParallel(cl)

set.seed(1984)
system.time(
  my_rf <- train(
    class ~ .,
    data = my_train,
    method = "rf",
    trControl = fit_control,
    tuneGrid = gbm_grid,
    metric = "ROC"
  )
)
stopCluster(cl)

my_rf$bestTune
```

Objects saved in `my_rf`.

```{r names_train_obj}
names(my_rf)
```

Final (best) model is saved.

```{r final_model}
my_rf$finalModel
```

Plot results.

```{r plot_my_rf}
ggplot(my_rf)
```

Best model according to area under the ROC.

```{r best_mtry}
slice_max(.data = my_rf$results, order_by = ROC, n = 1)
```

ROC metric on the testing data using the best model.

```{r auroc_test}
# column order matches class level
my_prob <- predict(my_rf$finalModel, my_test[, -ncol(my_test)], type = "prob")
pred <- prediction(my_prob[, 2], as.integer(my_test$class) - 1)

auc <- performance(pred, 'auc')
auc_value <- round(auc@y.values[[1]], 4)
perf <- performance(pred, 'tpr', 'fpr')

plot(perf, lwd = 3, colorize = TRUE)
legend("bottomright", legend = paste("AUC = ", auc_value))
```

The testing data had a much lower AUROC than the estimate from running `train`.

Assessment using the full dataset.

```{r train_my_rf_full}
ncore <- ceiling(detectCores() / 2)

cl <- makePSOCKcluster(ncore)
registerDoParallel(cl)

full_data <- my_feat_filt
full_data$class <- my_lab

set.seed(1984)
system.time(
  my_rf_full <- train(
    class ~ .,
    data = full_data,
    method = "rf",
    trControl = fit_control,
    tuneGrid = gbm_grid,
    metric = "ROC"
  )
)
stopCluster(cl)

my_rf_full$finalModel
```

Plot results.

```{r plot_my_rf_full}
ggplot(my_rf_full)
```

### Using `resamples`

Use the `resamples` function to manually test the `ntree` parameter.

```{r resamples}
fit_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10,
  p = 0.8,
  savePredictions = "final",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

ncore <- ceiling(detectCores() / 2)
cl <- makePSOCKcluster(ncore)
registerDoParallel(cl)
full_data <- my_feat_filt
full_data$class <- my_lab

my_models <- list()

for (n in seq(from = 500, to = 3000, by = 500)){
  set.seed(1984)
  system.time(
    my_rf <- train(
      class ~ .,
      data = full_data,
      method = "rf",
      trControl = fit_control,
      metric = "ROC",
      ntree = n
    )
  )
  my_models[[paste0('n', n)]] <- my_rf
}
stopCluster(cl)

my_res <- resamples(my_models)

summary(my_res)
```

Plot.

```{r resamples_dotplot}
dotplot(my_res)
```

Manually test two parameters.

```{r resamples_mtry_ntree}
fit_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10,
  p = 0.8,
  savePredictions = "final",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

ncore <- ceiling(detectCores() / 2)
cl <- makePSOCKcluster(ncore)
registerDoParallel(cl)
full_data <- my_feat_filt
full_data$class <- my_lab

my_models_m_n <- list()

max_mtry <- floor(sqrt(ncol(full_data)))

for (m in 1:max_mtry){
  gbm_grid <- expand.grid(
    mtry = m
  )
  for (n in seq(from = 500, to = 3000, by = 500)){
    set.seed(1984)
    system.time(
      my_rf <- train(
        class ~ .,
        data = full_data,
        method = "rf",
        trControl = fit_control,
        metric = "ROC",
        tuneGrid = gbm_grid,
        ntree = n
      )
    )
    my_models_m_n[[toString(paste0(n, '_', m))]] <- my_rf
  }
}

stopCluster(cl)

my_res_m_n <- resamples(my_models_m_n)
```

Best model.

```{r ntree_mtry_heatmap}
my_summary <- summary(my_res_m_n)
as_tibble(my_summary$statistics$ROC, rownames = "model") %>%
  tidyr::separate(col = model, into = c('ntree', 'mtry'), sep = "_") -> my_roc

ntree_lvl <- as.character(sort(unique(as.integer(pull(my_roc, ntree)))))

my_roc %>%
  mutate(ntree = factor(ntree, levels = ntree_lvl)) -> my_roc

ggplot(my_roc, aes(ntree, mtry)) +
  geom_tile(aes(fill = Mean)) +
  geom_text(aes(label = round(Mean, 4))) +
  scale_fill_gradient(low = "white", high = "red")
```

## Entire workflow

Implement the current workflow as a function to ease testing on a new data set (as long as the labels are stored in the last column).

```{r run_rf}
run_rf <- function(my_df, my_seed = 1984, max_mtry = 150, remove_nzv = TRUE, remove_cor = TRUE){
  my_feat <- my_df[, -ncol(my_df)]
  my_lab <- my_df[, ncol(my_df)]
  
  nzv <- nearZeroVar(my_feat)
  if(length(nzv) > 0 && remove_nzv){
    message(paste0("Removing ", length(nzv), " near zero variance features"))
    my_feat <- my_feat[, -nzv]
  }
  
  my_cor <- cor(my_feat)
  my_cor_feat <- findCorrelation(my_cor, cutoff = 0.90)
  if(length(my_cor_feat) > 0 && remove_cor){
    message(paste0("Removing ", length(my_cor_feat), " correlated features"))
    my_feat <- my_feat[, -my_cor_feat]
  }
  
  set.seed(my_seed)
  my_idx <- createDataPartition(
    y = my_lab,
    p = 0.8,
    list = FALSE,
    times = 1
  )
  my_train <- my_feat[my_idx, ]
  my_train$class <- my_lab[my_idx]
  my_test <- my_feat[-my_idx, ]
  my_test$class <- my_lab[-my_idx]
  
  fit_control <- trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 10,
    p = 0.8,
    savePredictions = "final",
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  )
  
  if(ncol(my_train) < max_mtry){
    max_mtry <- ncol(my_train)
  }
  
  gbm_grid <- expand.grid(
    mtry = 1:max_mtry
  )
  
  ncore <- ceiling(detectCores() / 2)
  
  cl <- makePSOCKcluster(ncore)
  registerDoParallel(cl)
  
  set.seed(my_seed)
  system.time(
    my_rf <- train(
      class ~ .,
      data = my_train,
      method = "rf",
      trControl = fit_control,
      tuneGrid = gbm_grid,
      metric = "ROC",
      verbose = FALSE
    )
  )
  stopCluster(cl)
  
  return(my_rf)
}
```

Load spam data and run workflow.

```{r run_rf_on_spam}
spam_data <- read.csv(file = "../data/spambase.csv")
spam_data$class <- factor(ifelse(spam_data$class == 1, 'S', 'H'))
spam_rf <- run_rf(spam_data)
spam_rf$bestTune
```

Plot tuning results.

```{r spam_rf_tune}
ggplot(spam_rf)
```

## Session info

Time built.

```{r time, echo=FALSE}
Sys.time()
```

Session info.

```{r session_info, echo=FALSE}
sessionInfo()
```
