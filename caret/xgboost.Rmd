---
title: "eXtreme Gradient Boosting"
output: md_document
---

```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_bw())
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = "img/")
```

## Introduction

See [XGBoost R Tutorial](https://xgboost.readthedocs.io/en/stable/R-package/xgboostPresentation.html) for more information. This README was generated by running this notebook in an RStudio server instance.

Install packages if missing and load.

```{r load_package, message=FALSE, warning=FALSE}
.libPaths('/packages')
my_packages <- c('doParallel', 'xgboost', 'plyr', 'mlbench', 'caret')

for (my_package in my_packages){
  if(!require(my_package, character.only = TRUE)){
    install.packages(my_package, '/packages')
  }
  library(my_package, character.only = TRUE)
}
```

## Sonar data

From `?Sonar`

> This is the data set used by Gorman and Sejnowski in their study of the classification of sonar signals using a neural network. The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.
>
> Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.
>
> The label associated with each record contains the letter "R" if the object is a rock and "M" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.

```{r prepare_data}
data(Sonar, package = "mlbench")

my_feat <- Sonar[, -ncol(Sonar)]
my_lab <- Sonar[, ncol(Sonar)]

dim(my_feat)
```

## Pre-processing

### Zero- and near zero-variance predictors

Predictors that have zero- or near zero-variance are not useful for making predictions since different classes will have the same values.

```{r nzv}
nzv <- nearZeroVar(my_feat, saveMetrics = TRUE)
table(nzv$nzv)
```

### Correlated predictors

The `findCorrelation` function flags correlated predictors for removal.

```{r descr_cor}
my_cor <- cor(my_feat)
summary(my_cor[lower.tri(my_cor)])
```

Remove highly correlated predictors.

```{r remove_cor}
my_cor_feat <- findCorrelation(my_cor, cutoff = 0.90)

my_feat_filt <- my_feat[, -my_cor_feat]
my_cor <- cor(my_feat_filt)
summary(my_cor[lower.tri(my_cor)])
```

Data dimension after removing correlated predictors.

```{r filtered_descr_no_cor}
dim(my_feat_filt)
```

### Data splitting

Separate into training (80%) and testing (20%) using `createDataPartition`. If the `y` argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data.

```{r create_data_part}
set.seed(1984)
my_idx <- createDataPartition(
  y = my_lab,
  p = 0.8,
  list = FALSE,
  times = 1
)

my_train <- my_feat_filt[my_idx, ]
my_train$class <- my_lab[my_idx]
my_test <- my_feat_filt[-my_idx, ]
my_test$class <- my_lab[-my_idx]
```

## Model training and tuning

The function `trainControl` can be used to specify the type of resampling and in the example below we perform 10 by 10 cross-validation. `classProbs` (logical) refers to whether class probabilities should be computed for classification models (along with predicted values) in each resample. `twoClassSummary` computes sensitivity, specificity and the area under the ROC curve.

Default grid for [DART booster](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html).

```{r default_grid}
# https://github.com/topepo/caret/blob/master/models/files/xgbDART.R
default_grid <- expand.grid(
  nrounds = floor((1:10) * 50),
  max_depth = seq(1, 10),
  eta = c(0.3, 0.4), # "Usually" one should be the lowest possible
  gamma = 0,
  subsample = seq(.5, 1, length = 10),
  colsample_bytree = c(0.6, 0.8),
  rate_drop = c(0.01, 0.50),
  skip_drop = c(0.05, 0.95),
  min_child_weight = c(1)
)
```

Train.

```{r dart_model_training_and_tuning}
ncore <- ceiling(detectCores() / 4)

cl <- makePSOCKcluster(ncore)
registerDoParallel(cl)

fit_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 1,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

my_grid <- expand.grid(
  nrounds = c(5, 10, 20, 50),
  max_depth = 6:7,
  eta = c(0.3, 0.4),
  gamma = 0,
  subsample = seq(.5, 1, length = 10),
  colsample_bytree = c(0.6, 0.8),
  rate_drop = c(0.01, 0.50),
  skip_drop = c(0.05, 0.95),
  min_child_weight = 1
)
dim(my_grid)

set.seed(1984)
system.time(
  my_xgb_dart <- train(
    class ~ .,
    data = my_train,
    method = "xgbDART",
    trControl = fit_control,
    metric = "ROC",
    tuneGrid = my_grid,
    nthread = 2
  )
)

stopCluster(cl)

arrange(my_xgb_dart$results, desc(ROC)) %>%
  select(ROC, nrounds, everything()) %>%
  head()
```

Train tree.

```{r tree_model_training_and_tuning}
ncore <- ceiling(detectCores() / 4)

cl <- makePSOCKcluster(ncore)
registerDoParallel(cl)

fit_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 1,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

my_grid <- expand.grid(
  nrounds = c(5, 10, 20, 50),
  max_depth = 6:7,
  eta = c(0.3, 0.4),
  gamma = 0,
  subsample = seq(.5, 1, length = 10),
  colsample_bytree = c(0.6, 0.8),
  min_child_weight = 1
)
dim(my_grid)

set.seed(1984)
system.time(
  my_xgb <- train(
    class ~ .,
    data = my_train,
    method = "xgbTree",
    trControl = fit_control,
    metric = "ROC",
    tuneGrid = my_grid,
    nthread = 2
  )
)

stopCluster(cl)

arrange(my_xgb$results, desc(ROC)) %>%
  select(ROC, nrounds, everything()) %>%
  head()
```

## Session info

Time built.

```{r time, echo=FALSE}
Sys.time()
```

Session info.

```{r session_info, echo=FALSE}
sessionInfo()
```
