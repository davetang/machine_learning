---
title: "eXtreme Gradient Boosting"
output: md_document
---

```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_bw())
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = "img/")
```

## Introduction

See [XGBoost R Tutorial](https://xgboost.readthedocs.io/en/stable/R-package/xgboostPresentation.html) for more information. This README was generated by running this notebook in an RStudio server instance.

Install packages if missing and load.

```{r load_package, message=FALSE, warning=FALSE}
.libPaths('/packages')
my_packages <- c('doParallel', 'xgboost', 'plyr', 'mlbench', 'caret')

for (my_package in my_packages){
  if(!require(my_package, character.only = TRUE)){
    install.packages(my_package, '/packages')
  }
  library(my_package, character.only = TRUE)
}
```

## Sonar data

From `?Sonar`

> This is the data set used by Gorman and Sejnowski in their study of the classification of sonar signals using a neural network. The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.
>
> Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.
>
> The label associated with each record contains the letter "R" if the object is a rock and "M" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.

```{r prepare_data}
data(Sonar, package = "mlbench")

my_feat <- Sonar[, -ncol(Sonar)]
my_lab <- Sonar[, ncol(Sonar)]

dim(my_feat)
```

## Pre-processing

### Zero- and near zero-variance predictors

Predictors that have zero- or near zero-variance are not useful for making predictions since different classes will have the same values.

```{r nzv}
nzv <- nearZeroVar(my_feat, saveMetrics = TRUE)
table(nzv$nzv)
```

### Correlated predictors

The `findCorrelation` function flags correlated predictors for removal.

```{r descr_cor}
my_cor <- cor(my_feat)
summary(my_cor[lower.tri(my_cor)])
```

Remove highly correlated predictors.

```{r remove_cor}
my_cor_feat <- findCorrelation(my_cor, cutoff = 0.90)

my_feat_filt <- my_feat[, -my_cor_feat]
my_cor <- cor(my_feat_filt)
summary(my_cor[lower.tri(my_cor)])
```

Data dimension after removing correlated predictors.

```{r filtered_descr_no_cor}
dim(my_feat_filt)
```

### Data splitting

Separate into training (80%) and testing (20%) using `createDataPartition`. If the `y` argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data.

```{r create_data_part}
set.seed(1984)
my_idx <- createDataPartition(
  y = my_lab,
  p = 0.8,
  list = FALSE,
  times = 1
)

my_train <- my_feat_filt[my_idx, ]
my_train$class <- my_lab[my_idx]
my_test <- my_feat_filt[-my_idx, ]
my_test$class <- my_lab[-my_idx]
```

## Model training and tuning

The caret package supports three different XGBoost models (which correspond to the type of booster):

1. `xgbDART` - used for classification and regression and tuning parameters include: nrounds, max_depth, eta, gamma, subsample, colsample_bytree, rate_drop, skip_drop, min_child_weight
2. `xgbLinear` - used for classification and regression, and tuning parameters include: nrounds, lambda, alpha, eta
3. `xgbTree` - used for classification and regression, and tuning parameters include: nrounds, max_depth, eta, gamma, colsample_bytree, min_child_weight, subsample

[Parameter descriptions](https://xgboost.readthedocs.io/en/stable/parameter.html):

* `num_round` (range: [0, Inf]) - The number of rounds for boosting
* `eta` (range: [0,1]) - a.k.a. the learning rate is a step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and `eta` shrinks the feature weights to make the boosting process more conservative.
* `gamma` (range: [0, Inf]) - the minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.
* `max_depth` (range: [0, Inf]) - the maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree.
* `min_child_weight` (range: [0, Inf]) - the minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than `min_child_weight`, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger `min_child_weight` is, the more conservative the algorithm will be.
* `subsample` (range: [0,1]) - the subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.
* `colsample_bytree` (range: [0,1]) - is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.
* `rate_drop` (range: [0,1]) - dropout rate (a fraction of previous trees to drop during the dropout).
* `skip_drop` (range: [0,1]) - probability of skipping the dropout procedure during a boosting iteration. If a dropout is skipped, new trees are added in the same manner as `gbtree`. Note that non-zero `skip_drop` has higher priority than `rate_drop` or `one_drop`.

Default grid for [DART booster](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html) ([xgbDART](https://github.com/topepo/caret/blob/master/models/files/xgbDART.R)).

```{r xgbdart_default_grid}
default_grid <- expand.grid(
  nrounds = floor((1:10) * 50),
  eta = c(0.3, 0.4),
  gamma = 0,
  max_depth = seq(1, 10),
  min_child_weight = c(1),
  subsample = seq(.5, 1, length = 10),
  colsample_bytree = c(0.6, 0.8),
  rate_drop = c(0.01, 0.50),
  skip_drop = c(0.05, 0.95)
)
```

The function `trainControl` can be used to specify the type of resampling and in the example below we perform 10 by 10 cross-validation. `classProbs` (logical) refers to whether class probabilities should be computed for classification models (along with predicted values) in each resample. `twoClassSummary` computes sensitivity, specificity and the area under the ROC curve.

```{r train_control}
fit_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 1,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
```

Train with DART Booster.

```{r dart_model_training_and_tuning}
ncore <- 16
cl <- makePSOCKcluster(ncore)
registerDoParallel(cl)
my_grid <- expand.grid(
  nrounds = c(5, 10, 20, 50),
  max_depth = 6:7,
  eta = c(0.3, 0.4),
  gamma = 0,
  subsample = seq(.5, 1, length = 10),
  colsample_bytree = c(0.6, 0.8),
  rate_drop = c(0.01, 0.50),
  skip_drop = c(0.05, 0.95),
  min_child_weight = 1
)

set.seed(1984)
system.time(
  my_xgbdart <- train(
    class ~ .,
    data = my_train,
    method = "xgbDART",
    trControl = fit_control,
    metric = "ROC",
    tuneGrid = my_grid,
    nthread = 2
  )
)
stopCluster(cl)

arrange(my_xgbdart$results, desc(ROC)) %>%
  select(ROC, nrounds, everything()) %>%
  head()
```

Train using `xgbTree`.

```{r tree_model_training_and_tuning}
ncore <- 16
cl <- makePSOCKcluster(ncore)
registerDoParallel(cl)

fit_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 1,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

my_grid <- expand.grid(
  nrounds = c(5, 10, 20, 50),
  max_depth = 6:7,
  eta = c(0.3, 0.4),
  gamma = 0,
  subsample = seq(.5, 1, length = 10),
  colsample_bytree = c(0.6, 0.8),
  min_child_weight = 1
)

set.seed(1984)
system.time(
  my_xgbtree <- train(
    class ~ .,
    data = my_train,
    method = "xgbTree",
    trControl = fit_control,
    metric = "ROC",
    tuneGrid = my_grid,
    nthread = 2
  )
)
stopCluster(cl)

arrange(my_xgbtree$results, desc(ROC)) %>%
  select(ROC, nrounds, everything()) %>%
  head()
```

Linear Booster parameters.

* `lambda` - L2 regularization term on weights. Increasing this value will make model more conservative. Normalised to number of training examples.
* `alpha` - L1 regularization term on weights. Increasing this value will make model more conservative. Normalised to number of training examples.

Train using `xgbLinear` - used for classification and regression, and tuning parameters include: nrounds, lambda, alpha, eta

```{r linear_model_training_and_tuning}
ncore <- 16
cl <- makePSOCKcluster(ncore)
registerDoParallel(cl)

fit_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 1,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

my_grid <- expand.grid(
  nrounds = c(5, 10, 20, 50),
  eta = c(0.3, 0.4),
  lambda = c(0.3, 0.6, 1, 2, 3, 5),
  alpha = c(0.3, 0.6, 1, 2, 3, 5)
)

set.seed(1984)
system.time(
  my_xgblinear <- train(
    class ~ .,
    data = my_train,
    method = "xgbLinear",
    trControl = fit_control,
    metric = "ROC",
    tuneGrid = my_grid,
    nthread = 2
  )
)
stopCluster(cl)

arrange(my_xgblinear$results, desc(ROC)) %>%
  select(ROC, nrounds, everything()) %>%
  head()
```

## Comparing models

Collect resampling results using `resamples`.

```{r resamples}
resamps <- resamples(
  list(
    DART = my_xgbdart,
    Linear = my_xgblinear,
    Tree = my_xgbtree
  )
)
resamps
```

Summary.

```{r resamps_summary}
summary(resamps)
```

Boxplots.

```{r bwplot}
bwplot(resamps, layout = c(3, 1))
```

Compute differences and use a simple _t_-test to evaluate the null hypothesis that there is no difference between models.

```{r dif_values}
diff_values <- diff(resamps)
summary(diff_values)
```

Boxplots.

```{r bwplot_diff_values}
bwplot(diff_values, layout = c(3, 1))
```

## Session info

Time built.

```{r time, echo=FALSE}
Sys.time()
```

Session info.

```{r session_info, echo=FALSE}
sessionInfo()
```
