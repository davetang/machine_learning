## Introduction

See [XGBoost R
Tutorial](https://xgboost.readthedocs.io/en/stable/R-package/xgboostPresentation.html)
for more information. This README was generated by running this notebook
in an RStudio server instance.

Install packages if missing and load.

    .libPaths('/packages')
    my_packages <- c('doParallel', 'xgboost', 'plyr', 'mlbench', 'caret')

    for (my_package in my_packages){
      if(!require(my_package, character.only = TRUE)){
        install.packages(my_package, '/packages')
      }
      library(my_package, character.only = TRUE)
    }

## Sonar data

From `?Sonar`

> This is the data set used by Gorman and Sejnowski in their study of
> the classification of sonar signals using a neural network. The task
> is to train a network to discriminate between sonar signals bounced
> off a metal cylinder and those bounced off a roughly cylindrical rock.
>
> Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each
> number represents the energy within a particular frequency band,
> integrated over a certain period of time. The integration aperture for
> higher frequencies occur later in time, since these frequencies are
> transmitted later during the chirp.
>
> The label associated with each record contains the letter “R” if the
> object is a rock and “M” if it is a mine (metal cylinder). The numbers
> in the labels are in increasing order of aspect angle, but they do not
> encode the angle directly.

    data(Sonar, package = "mlbench")

    my_feat <- Sonar[, -ncol(Sonar)]
    my_lab <- Sonar[, ncol(Sonar)]

    dim(my_feat)

    ## [1] 208  60

## Pre-processing

### Zero- and near zero-variance predictors

Predictors that have zero- or near zero-variance are not useful for
making predictions since different classes will have the same values.

    nzv <- nearZeroVar(my_feat, saveMetrics = TRUE)
    table(nzv$nzv)

    ## 
    ## FALSE 
    ##    60

### Correlated predictors

The `findCorrelation` function flags correlated predictors for removal.

    my_cor <- cor(my_feat)
    summary(my_cor[lower.tri(my_cor)])

    ##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
    ## -0.53178 -0.10560  0.10370  0.09967  0.25883  0.92584

Remove highly correlated predictors.

    my_cor_feat <- findCorrelation(my_cor, cutoff = 0.90)

    my_feat_filt <- my_feat[, -my_cor_feat]
    my_cor <- cor(my_feat_filt)
    summary(my_cor[lower.tri(my_cor)])

    ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    ## -0.5318 -0.0935  0.1095  0.1047  0.2588  0.8992

Data dimension after removing correlated predictors.

    dim(my_feat_filt)

    ## [1] 208  57

### Data splitting

Separate into training (80%) and testing (20%) using
`createDataPartition`. If the `y` argument to this function is a factor,
the random sampling occurs within each class and should preserve the
overall class distribution of the data.

    set.seed(1984)
    my_idx <- createDataPartition(
      y = my_lab,
      p = 0.8,
      list = FALSE,
      times = 1
    )

    my_train <- my_feat_filt[my_idx, ]
    my_train$class <- my_lab[my_idx]
    my_test <- my_feat_filt[-my_idx, ]
    my_test$class <- my_lab[-my_idx]

## Model training and tuning

The function `trainControl` can be used to specify the type of
resampling and in the example below we perform 10 by 10
cross-validation. `classProbs` (logical) refers to whether class
probabilities should be computed for classification models (along with
predicted values) in each resample. `twoClassSummary` computes
sensitivity, specificity and the area under the ROC curve.

Default grid for [DART
booster](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html).

    # https://github.com/topepo/caret/blob/master/models/files/xgbDART.R
    default_grid <- expand.grid(
      nrounds = floor((1:10) * 50),
      max_depth = seq(1, 10),
      eta = c(0.3, 0.4), # "Usually" one should be the lowest possible
      gamma = 0,
      subsample = seq(.5, 1, length = 10),
      colsample_bytree = c(0.6, 0.8),
      rate_drop = c(0.01, 0.50),
      skip_drop = c(0.05, 0.95),
      min_child_weight = c(1)
    )

Train.

    ncore <- ceiling(detectCores() / 4)

    cl <- makePSOCKcluster(ncore)
    registerDoParallel(cl)

    fit_control <- trainControl(
      method = "repeatedcv",
      number = 5,
      repeats = 1,
      classProbs = TRUE,
      summaryFunction = twoClassSummary
    )

    my_grid <- expand.grid(
      nrounds = c(5, 10, 20, 50),
      max_depth = 6:7,
      eta = c(0.3, 0.4),
      gamma = 0,
      subsample = seq(.5, 1, length = 10),
      colsample_bytree = c(0.6, 0.8),
      rate_drop = c(0.01, 0.50),
      skip_drop = c(0.05, 0.95),
      min_child_weight = 1
    )
    dim(my_grid)

    ## [1] 1280    9

    set.seed(1984)
    system.time(
      my_xgb_dart <- train(
        class ~ .,
        data = my_train,
        method = "xgbDART",
        trControl = fit_control,
        metric = "ROC",
        tuneGrid = my_grid,
        nthread = 2
      )
    )

    ##    user  system elapsed 
    ##  11.871  12.745 328.780

    stopCluster(cl)

    arrange(my_xgb_dart$results, desc(ROC)) %>%
      select(ROC, nrounds, everything()) %>%
      head()

    ##         ROC nrounds max_depth eta rate_drop skip_drop min_child_weight
    ## 1 0.9446623      50         6 0.4      0.01      0.05                1
    ## 2 0.9416939      50         6 0.3      0.01      0.05                1
    ## 3 0.9406454      50         6 0.3      0.01      0.95                1
    ## 4 0.9383034      50         7 0.4      0.01      0.95                1
    ## 5 0.9376634      20         7 0.4      0.01      0.05                1
    ## 6 0.9375054      50         7 0.4      0.01      0.05                1
    ##   subsample colsample_bytree gamma      Sens      Spec      ROCSD     SensSD
    ## 1 0.8888889              0.8     0 0.9091503 0.8100000 0.05267170 0.05356886
    ## 2 0.5555556              0.8     0 0.8653595 0.8600000 0.02703605 0.02935725
    ## 3 0.8888889              0.6     0 0.8647059 0.7841667 0.02581661 0.05149522
    ## 4 0.8888889              0.6     0 0.8869281 0.8358333 0.04464540 0.04283415
    ## 5 0.9444444              0.6     0 0.8980392 0.7958333 0.04051349 0.06363745
    ## 6 0.9444444              0.6     0 0.9098039 0.8091667 0.04662337 0.05050051
    ##      SpecSD
    ## 1 0.1448359
    ## 2 0.1194097
    ## 3 0.1545996
    ## 4 0.1117102
    ## 5 0.1187317
    ## 6 0.1147688

Train tree.

    ncore <- ceiling(detectCores() / 4)

    cl <- makePSOCKcluster(ncore)
    registerDoParallel(cl)

    fit_control <- trainControl(
      method = "repeatedcv",
      number = 5,
      repeats = 1,
      classProbs = TRUE,
      summaryFunction = twoClassSummary
    )

    my_grid <- expand.grid(
      nrounds = c(5, 10, 20, 50),
      max_depth = 6:7,
      eta = c(0.3, 0.4),
      gamma = 0,
      subsample = seq(.5, 1, length = 10),
      colsample_bytree = c(0.6, 0.8),
      min_child_weight = 1
    )
    dim(my_grid)

    ## [1] 320   7

    set.seed(1984)
    system.time(
      my_xgb <- train(
        class ~ .,
        data = my_train,
        method = "xgbTree",
        trControl = fit_control,
        metric = "ROC",
        tuneGrid = my_grid,
        nthread = 2
      )
    )

    ##    user  system elapsed 
    ##   9.258   6.410  89.267

    stopCluster(cl)

    arrange(my_xgb$results, desc(ROC)) %>%
      select(ROC, nrounds, everything()) %>%
      head()

    ##         ROC nrounds eta max_depth gamma colsample_bytree min_child_weight
    ## 1 0.9442593      50 0.4         6     0              0.8                1
    ## 2 0.9381400      50 0.4         7     0              0.8                1
    ## 3 0.9356863      50 0.4         7     0              0.6                1
    ## 4 0.9355692      50 0.4         7     0              0.8                1
    ## 5 0.9335240      50 0.3         7     0              0.8                1
    ## 6 0.9329548      50 0.3         7     0              0.8                1
    ##   subsample      Sens      Spec      ROCSD     SensSD     SpecSD
    ## 1 0.5555556 0.8875817 0.8225000 0.04126329 0.05563240 0.12640186
    ## 2 0.7222222 0.8647059 0.8100000 0.05556162 0.05149522 0.15774562
    ## 3 0.7222222 0.8535948 0.8600000 0.03250095 0.03246663 0.10174689
    ## 4 0.6666667 0.8764706 0.8208333 0.02674169 0.02427997 0.09218603
    ## 5 0.5000000 0.8980392 0.8608333 0.04996001 0.05006510 0.13435804
    ## 6 0.6666667 0.8640523 0.7966667 0.01507364 0.06802568 0.11881942

## Session info

Time built.

    ## [1] "2022-11-09 07:53:07 UTC"

Session info.

    ## R version 4.2.0 (2022-04-22)
    ## Platform: x86_64-pc-linux-gnu (64-bit)
    ## Running under: Ubuntu 20.04.4 LTS
    ## 
    ## Matrix products: default
    ## BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
    ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3
    ## 
    ## locale:
    ##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
    ##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
    ##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
    ##  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
    ##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
    ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       
    ## 
    ## attached base packages:
    ## [1] parallel  stats     graphics  grDevices utils     datasets  methods  
    ## [8] base     
    ## 
    ## other attached packages:
    ##  [1] caret_6.0-93      lattice_0.20-45   mlbench_2.1-3     plyr_1.8.7       
    ##  [5] xgboost_1.6.0.1   doParallel_1.0.17 iterators_1.0.14  foreach_1.5.2    
    ##  [9] forcats_0.5.2     stringr_1.4.1     dplyr_1.0.10      purrr_0.3.5      
    ## [13] readr_2.1.3       tidyr_1.2.1       tibble_3.1.8      ggplot2_3.4.0    
    ## [17] tidyverse_1.3.2  
    ## 
    ## loaded via a namespace (and not attached):
    ##  [1] nlme_3.1-160         fs_1.5.2             lubridate_1.9.0     
    ##  [4] httr_1.4.4           tools_4.2.0          backports_1.4.1     
    ##  [7] utf8_1.2.2           R6_2.5.1             rpart_4.1.19        
    ## [10] DBI_1.1.3            colorspace_2.0-3     nnet_7.3-18         
    ## [13] withr_2.5.0          tidyselect_1.2.0     compiler_4.2.0      
    ## [16] cli_3.4.1            rvest_1.0.3          xml2_1.3.3          
    ## [19] scales_1.2.1         digest_0.6.30        rmarkdown_2.17      
    ## [22] pkgconfig_2.0.3      htmltools_0.5.3      parallelly_1.32.1   
    ## [25] dbplyr_2.2.1         fastmap_1.1.0        rlang_1.0.6         
    ## [28] readxl_1.4.1         rstudioapi_0.14      generics_0.1.3      
    ## [31] jsonlite_1.8.3       ModelMetrics_1.2.2.2 googlesheets4_1.0.1 
    ## [34] magrittr_2.0.3       Matrix_1.5-1         Rcpp_1.0.9          
    ## [37] munsell_0.5.0        fansi_1.0.3          lifecycle_1.0.3     
    ## [40] pROC_1.18.0          stringi_1.7.8        yaml_2.3.6          
    ## [43] MASS_7.3-58.1        recipes_1.0.2        grid_4.2.0          
    ## [46] listenv_0.8.0        crayon_1.5.2         haven_2.5.1         
    ## [49] splines_4.2.0        hms_1.1.2            knitr_1.40          
    ## [52] pillar_1.8.1         stats4_4.2.0         reshape2_1.4.4      
    ## [55] future.apply_1.10.0  codetools_0.2-18     reprex_2.0.2        
    ## [58] glue_1.6.2           evaluate_0.17        data.table_1.14.4   
    ## [61] modelr_0.1.9         vctrs_0.5.0          tzdb_0.3.0          
    ## [64] cellranger_1.1.0     gtable_0.3.1         future_1.29.0       
    ## [67] assertthat_0.2.1     xfun_0.34            gower_1.0.0         
    ## [70] prodlim_2019.11.13   broom_1.0.1          class_7.3-20        
    ## [73] survival_3.4-0       googledrive_2.0.0    gargle_1.2.1        
    ## [76] timeDate_4021.106    hardhat_1.2.0        lava_1.7.0          
    ## [79] timechange_0.1.1     globals_0.16.1       ellipsis_0.3.2      
    ## [82] ipred_0.9-13
