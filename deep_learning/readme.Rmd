---
title: "Getting started with Keras"
output:
  github_document:
    html_preview: false
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_bw())
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = "img/")
```

# Keras

[Keras](https://keras.posit.co/) is a high-level neural networks API developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Keras has the following key features:

* Allows the same code to run on CPU or on GPU, seamlessly.
* User-friendly API which makes it easy to quickly prototype deep learning models.
* Built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.
* Supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.

## Packages

Install packages if missing and load.

```{r load_package, message=FALSE, warning=FALSE}
.libPaths('/packages')
my_packages <- c('keras3', 'tensorflow')

for (my_package in my_packages){
   if(!require(my_package, character.only = TRUE)){
      install.packages(my_package, '/packages')
      library(my_package, character.only = TRUE)
   }
}
```

## Reticulate

Use [reticulate](https://rstudio.github.io/reticulate/).

```{r load_reticulate}
library(reticulate)
use_python("/usr/bin/python3")
reticulate::py_config()
```

## MNIST dataset

[Simple example](https://keras.posit.co/articles/getting_started.html#mnist-example) of trying to recognise handwritten digits from the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits.

The MNIST dataset is included with Keras and can be accessed using the `dataset_mnist()` function.

```{r dataset_mnist}
mnist <- suppressMessages(dataset_mnist())
str(mnist)
```
The `x` data is a 3-d array (images, width, height) of grayscale values.

```{r digit_heatmap}
idx <- 1984
image(mnist$train$x[idx,,], main = mnist$train$y[idx])
```

Store training and testing data.

```{r store_train_test_data}
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```

To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1.

```{r}
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255

str(x_train)
```

The y data is an integer vector with values ranging from 0 to 9. To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras `to_categorical()` function.

```{r}
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)

str(y_train)
```

The core data structure of Keras is a model, a way to organise layers. The simplest type of model is the Sequential model, a linear stack of layers.

We begin by creating a sequential model and then adding layers using the pipe (|>) operator.

```{r keras_model}
model <- keras_model_sequential(input_shape = c(784))
model |>
  layer_dense(units = 256, activation = 'relu') |>
  layer_dropout(rate = 0.4) |>
  layer_dense(units = 128, activation = 'relu') |>
  layer_dropout(rate = 0.3) |>
  layer_dense(units = 10, activation = 'softmax')
```

The input_shape argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a softmax activation function.

Use the `summary()` function to print the details of the model.

```{r model_summary}
summary(model)
```

Plot.

```{r model_plot}
plot(model)
```

Next, compile the model with appropriate loss function, optimiser, and metrics.

```{r model_compile}
model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

Use the `fit()` function to train the model for 30 epochs using batches of 128 images.

```{r model_fit}
history <- model |> fit(
  x_train,
  y_train,
  epochs = 30,
  batch_size = 128,
  validation_split = 0.2
)
```

The history object returned by `fit()` includes loss and accuracy metrics which we can plot.

```{r}
plot(history)
```

Evaluate the model's performance on the test data.

```{r model_evaluate}
model |> evaluate(x_test, y_test)
```

Generate predictions on new data x.

```{r model_predict}
probs <- model |> predict(x_test)
```

Predictions.

```{r predictions}
head(max.col(probs) - 1L)
```

Truth.

```{r test_labels}
head(mnist$test$y)
```

Double check accuracy.

```{r}
sum(max.col(probs) - 1L == mnist$test$y) / length(mnist$test$y)
```

## Fashion MNIST

Following the [Basic Image Classification](https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_classification/) guide.

This guide uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels).

Fashion MNIST is intended as a drop-in replacement for the classic MNIST dataset and is a slightly more challenging problem than regular MNIST.

We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images.

```{r fashion_mnist}
fashion_mnist <- dataset_fashion_mnist()

str(fashion_mnist)
```

Assign.

```{r fashion_mnist_data}
c(train_images, train_labels) %<-% fashion_mnist$train
c(test_images, test_labels) %<-% fashion_mnist$test
```

The labels are arrays of integers ranging from 0 to 9 and correspond to the class of clothing the image represents:

| Digit |    Class    |
| ----- | ----------- |
|   0   | T-shirt/top |
|   1   | Trouser     |
|   2   | Pullover    |
|   3   | Dress       |
|   4   | Coat        |
|   5   | Sandal      |
|   6   | Shirt       |
|   7   | Sneaker     |
|   8   | Bag         |
|   9   | Ankle boot  |

```{r class_names}
class_names = c(
   'T-shirt/top',
   'Trouser',
   'Pullover',
   'Dress',
   'Coat', 
   'Sandal',
   'Shirt',
   'Sneaker',
   'Bag',
   'Ankle boot'
)
```

```{r}
image_1 <- as.data.frame(train_images[1, , ])
colnames(image_1) <- seq_len(ncol(image_1))
image_1$y <- seq_len(nrow(image_1))
image_1 <- gather(image_1, "x", "value", -y)
image_1$x <- as.integer(image_1$x)

ggplot(image_1, aes(x = x, y = y, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "black", na.value = NA) +
  scale_y_reverse() +
  theme_minimal() +
  theme(panel.grid = element_blank())   +
  theme(aspect.ratio = 1) +
  xlab("") +
  ylab("")
```

Scale values to a range of 0 to 1 by dividing by 255 before feeding to the neural network model. It is important that the training set and the testing set are pre-processed in the same way.

```{r}
train_images <- train_images / 255
test_images <- test_images / 255
```

Display the first 25 images.

```{r}
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) { 
  img <- train_images[i, , ]
  img <- t(apply(img, 2, rev)) 
  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
        main = paste(class_names[train_labels[i] + 1]))
}
```

Set up the layers.

```{r}
model <- keras_model_sequential()
model %>%
  layer_flatten(input_shape = c(28, 28)) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')
```

```{r}
model %>% compile(
  optimizer = 'adam', 
  loss = 'sparse_categorical_crossentropy',
  metrics = c('accuracy')
)
```

```{r}
model %>% fit(train_images, train_labels, epochs = 5, verbose = 2)
```

```{r}
score <- model %>% evaluate(test_images, test_labels, verbose = 0)
score
```

```{r}
predictions <- model %>% predict(test_images)
predictions[1, ]
```
```{r}
which.max(predictions[1, ])
```

```{r eval=FALSE}
class_pred <- model %>% predict(test_images) %>% k_argmax()

class_pred[1:20]
```

```{r}
test_labels[1:20]
```

```{r}
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) { 
   img <- test_images[i, , ]
   img <- t(apply(img, 2, rev)) 
   # subtract 1 as labels go from 0 to 9
   predicted_label <- which.max(predictions[i, ]) - 1
   true_label <- test_labels[i]
   if (predicted_label == true_label) {
      color <- '#008800' 
   } else {
      color <- '#bb0000'
   }
   image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
         main = paste0(class_names[predicted_label + 1], " (",
                       class_names[true_label + 1], ")"),
         col.main = color)
}
```

## Literature

* [Review of deep learning: concepts, CNN architectures, challenges, applications, future directions](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8010506/)

## Session info

Time built.

```{r time, echo=FALSE}
Sys.time()
```

Session info.

```{r session_info, echo=FALSE}
sessionInfo()
```
