# Transformer

From ChatGPT.

The Transformer architecture is a type of deep learning model that was introduced in the paper "[Attention is All You Need](https://arxiv.org/abs/1706.03762)" by Vaswani et al. in 2017. It has become widely used in various natural language processing (NLP) tasks and has also been adapted for other sequence-to-sequence tasks like image captioning and speech recognition.

The Transformer architecture relies heavily on the attention mechanism, which enables the model to focus on different parts of the input sequence when processing it. Here's an overview of the key components of the Transformer architecture:

1. **Self-Attention Mechanism**:
    - The self-attention mechanism allows the model to weigh the importance of different words in the input sequence when generating each output word.
    - At each position in the sequence, the model computes attention scores between that position and every other position in the sequence.
    - These attention scores are used to compute a weighted sum of the input embeddings, where the weights are determined by the attention scores.
    - This mechanism allows the model to capture dependencies between words in the input sequence and generate contextually relevant representations.

2. **Multi-Head Attention**:
    - To capture different types of information and attend to different parts of the input sequence simultaneously, the Transformer architecture uses multiple attention heads.
    - Each attention head independently computes attention scores and produces its own output representation.
    - The outputs of the attention heads are concatenated and linearly transformed to produce the final output of the multi-head attention layer.

3. **Positional Encoding**:
    - Since the Transformer architecture does not inherently capture the order of words in the input sequence, positional encodings are added to the input embeddings to provide information about the position of each word.
    - Positional encodings are learned embeddings that encode the position of each word in the input sequence using sinusoidal functions.

4. **Encoder and Decoder Stacks**:
    - The Transformer architecture consists of a stack of encoder layers and a stack of decoder layers.
    - The encoder stack processes the input sequence and generates a sequence of hidden representations.
    - The decoder stack takes the hidden representations generated by the encoder and produces the output sequence, one word at a time.

5. **Feed-Forward Neural Networks**:
    - Each encoder and decoder layer in the Transformer architecture contains feed-forward neural networks (FFNs).
    - The FFNs apply a linear transformation followed by a non-linear activation function (usually ReLU) to each position in the sequence independently.

6. **Layer Normalisation and Residual Connections**:
    - To stabilise the training process and facilitate the flow of gradients, layer normalisation and residual connections are applied after each sub-layer in the encoder and decoder stacks.

Overall, the Transformer architecture has revolutionised the field of natural language processing by providing a more parallelisable and scalable approach to sequence modeling compared to recurrent neural networks (RNNs) and convolutional neural networks (CNNs). Its attention-based mechanism allows it to capture long-range dependencies in sequences and achieve state-of-the-art performance on various NLP tasks.
