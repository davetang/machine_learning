---
title: "Principal Components Analysis"
output:
   md_document:
      variant: markdown
---

```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_bw())
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = "img/")
```

## Introduction

This README was generated by running from the root directory of this repository:

    script/rmd_to_md.sh pca/readme.Rmd

## PCA background

Sample data as per PCA tutorial.

```{r x}
X = c(1, 2, 4, 6, 12, 15, 25, 45, 68, 67, 65, 98)
```

Sample mean (X bar) by adding up all the numbers and then divide by how many there are.

$$ \bar{X}  = \frac{\sum^n_{i=1} X_i}{n}$$

```{r x_bar}
mean(X)
```

The Standard Deviation (SD) of a data set is a measure of how spread out the data is. The way to calculate it is to compute the squares of the distance from each data point to the mean of the set, add them all up, divide by $n-1$, and take the positive square root.

$$ s = \sqrt\frac{\sum^n_{i=1} (X_i - \bar{X})^2}{(n - 1)} $$

```{r x_sd}
sd(X)
```
Variance is another measure of the spread of data in a data set and it is simply the standard deviation squared.

$$ s^2 = \frac{\sum^n_{i=1} (X_i - \bar{X})^2}{(n - 1)} $$

Many data sets have more than one dimension and the aim of the statistical analysis of these data sets is usually to see if there is any relationship between the dimensions. Standard deviation and variance only operate on one dimension, so you can only calculate the standard deviation for each dimension of the data set _independent_ of the other dimensions. However, it is useful to have a similar measure to find out how much the dimensions vary from the mean _with respect to each other_, which is what covariance measures and is calculated _between_ two dimensions.

If you calculate the covariance between one dimension and _itself_, you end up with the variance. For a 3D data set (x, y, z), you could measure the covariance between the x and y dimensions, the x and z dimensions, and the y and z dimensions. The formula for covariance is very similar to the formula for variance.


$$ cov(X, Y) = \frac{\sum^n_{i=1} (X_i-\bar{X})(Y_i-\bar{Y})}{(n - 1)} $$

How does the covariance work? Imagine a 2D data set containing hours spent studying for an exam (`hours`) and the mark they received for the exam (`mark`).

```{r hours_vs_mark}
my_df <- data.frame(
   hours = c(9, 15, 25, 14, 10, 18, 0, 16, 5, 19, 16, 20),
   mark = c(39, 56, 93, 61, 50, 75, 32, 85, 42, 70, 66, 80)
)

cov(my_df)
```

The exact value is not as important as it's sign (i.e. positive or negative). If the value is positive, then it indicates that both dimensions _increase together_, meaning that as the numbers of hours of study increased, so did the mark.

If the value is negative, such as the example below, then one dimension increases as the other decreases; the windier it gets, the lower the temperature.

```{r airquality}
cov(airquality[, c('Wind', 'Temp')])
```

If the covariance is zero (or close to zero), then it indicates that the two dimensions are independent of each other.

```{r random_cov}
my_random <- data.frame(
   x = rnorm(20),
   y = rnorm(20)
)

cov(my_random)
```

A useful way to represent all possible covariance values in to calculate them and store them in a matrix.

```{r cov_matrix}
cov(airquality[, 1:4], use = "complete.ob")
```

You can multiply two matrices together provided that they are of compatible sizes and eigenvectors are a special case of this.

The following multiplication, results in a vector that is not an integer multiple of the original vector, i.e. non-eigenvector.

$$ \begin{bmatrix} 2 & 3 \\ 2 & 1 \end{bmatrix} \times \begin{bmatrix} 1 \\ 3 \end{bmatrix} = \begin{bmatrix} 11 \\ 5 \end{bmatrix} $$

This second example, results in a vector that is exactly four times the vector we began with, i.e. an eigenvector.

$$ \begin{bmatrix} 2 & 3 \\ 2 & 1 \end{bmatrix} \times \begin{bmatrix} 3 \\ 2 \end{bmatrix} = \begin{bmatrix} 12 \\ 8 \end{bmatrix} = 4 \times \begin{bmatrix} 3 \\ 2 \end{bmatrix} $$

In R.

```{r matrix_mut}
matrix(c(2, 3, 2, 1), byrow = TRUE, nrow = 2) %*% matrix(c(3, 2))
```

The vector $ \begin{bmatrix} 3 \\ 2 \end{bmatrix} $ represents an arrow pointing from the origin, $ (0,0) $, to the point $ (3,2) $. The square matrix $ \begin{bmatrix} 2 & 3 \\ 2 & 1 \end{bmatrix} $ can be considered as a transformation matrix. If you multiply this matrix on the left of a vector (as per the example), the answer is another vector that is transformed from its original position. It is the nature of the transformation that the eigenvectors arise from.

Now imagine a transformation matrix that, when multiplied on the left, reflected vectors in the line $y = x$ (like the second example). Then you can see that if there were a vector that lay on the line $y = x$, it's reflection is itself. This vector (and all multiples of it), would be an eigenvector of that transformation matrix.

Eigenvectors can only be found for square matrices and not every square matrix has eigenvectors. If an $n \times n$ matrix does have eigenvectors, there are $n$ of them. Lastly, all eigenvectors of a matrix are perpendicular, i.e. at right angles to each other, no matter the number of dimensions. Another word for perpendicular is orthogonal and being orthogonal is important because it means that you can express the data in terms of these perpendicular eigenvectors, instead of expressing them in terms of the $x$ and $y$ axes.

In addition, eigenvectors are scaled such that it has a length of 1, so that all eigenvectors have the same length. The vector $ \begin{bmatrix} 3 \\ 2 \end{bmatrix} $ has a length $ \sqrt(3^2 + 2^2) = \sqrt13 $ so we divide the original vector by this length to make it have a length of 1.

```{r standardise_eigenvector}
matrix(c(3,2) / sqrt(13))
```

4 is the eigenvalue associated with the $ \begin{bmatrix} 3 \\ 2 \end{bmatrix} $ and the `eigen()` function can be used to find eigenvalues and eigenvectors of a square matrix.

```{r eigen}
my_mat <- matrix(c(2, 3, 2, 1), byrow = TRUE, nrow = 2)
eigen(my_mat)
```

## PCA

Principal Components Analysis (PCA) is a way of identifying patterns in data and expressing the data in such a way as to highlight their similarities and differences.

```{r pca_data}
x <- c(2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2, 1, 1.5, 1.1)
y <- c(2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9)
```

## Breast cancer data

Using the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).

```{r prepare_data}
data <- read.table(
   "../data/breast_cancer_data.csv",
   stringsAsFactors = FALSE,
   sep = ',',
   header = TRUE
)
data$class <- factor(data$class)
data <- data[,-1]
```

Separate into training (80%) and testing (20%).

```{r split_data}
set.seed(31)
my_prob <- 0.8
my_split <- as.logical(
  rbinom(
    n = nrow(data),
    size = 1,
    p = my_prob
  )
)

train <- data[my_split,]
test <- data[!my_split,]
```

## Results

```{r plot}
ggplot(data, aes(class, ucsize)) +
   geom_boxplot()
```

## Session info

Time built.

```{r time, echo=FALSE}
Sys.time()
```

Session info.

```{r session_info, echo=FALSE}
sessionInfo()
```

