---
title: "Getting started with tidymodels"
output:
   md_document:
      variant: markdown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = "img/")
```

## Setup

Install packages.

```{r load_packages, message=FALSE, warning=FALSE}
my_packages <- c('tidyverse', 'tidymodels', 'randomForest', 'ROCR')

for (my_package in my_packages){
   if(!require(my_package, character.only = TRUE)){
      install.packages(my_package)
   }
  library(my_package, character.only = TRUE)
}

theme_set(theme_bw())
```

## Spam

Use [spam data](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names) to train a Random Forest model to illustrate evaluation measures.

```{r load_spam}
spam_data <- read.csv(file = "../data/spambase.csv")
dim(spam_data)
```

Class 0 and 1 are ham (non-spam) and spam, respectively.
 
```{r preview_spam}
spam_data$class <- factor(spam_data$class)
spam_data[c(1:3, (nrow(spam_data)-2):nrow(spam_data)), (ncol(spam_data)-2):ncol(spam_data)]
```

[Split data](https://www.tidymodels.org/start/resampling/#data-split) using [rsample](https://rsample.tidymodels.org/).

The `initial_split()` function takes the original data and saves the information on how to make the partitions. The `strata` argument conducts a stratified split ensuring that our training and test data sets will keep roughly the same proportion of classes.

```{r split_spam}
set.seed(1984)
spam_split <- initial_split(data = spam_data, prop = 0.8, strata = 'class')
spam_split
spam_train <- training(spam_split)
spam_test <- testing(spam_split)
```

## `parsnip`

The [parsnip package](https://parsnip.tidymodels.org/index.html) provides a tidy and unified interface to a range of models.

```{r train_rf}
my_mtry <- ceiling(sqrt(ncol(spam_data)))

rf <- list()
rand_forest(mtry = my_mtry, trees = 500) |>
  set_engine("randomForest") |>
  set_mode("classification") -> rf$model

rf$model |>
  fit(class ~ ., data = spam_train) -> rf$fit

rf$model
```

`rf` contains the model parameters and the model.

```{r str_rf}
str(rf, max.level = 2)
```

## `yardstick`

The [yardstick package](https://yardstick.tidymodels.org/) provides a tidy interface to estimate how well models are performing.

Example data to check how to prepare our data for use with `yardstick`.

```{r two_class_example}
data(two_class_example)
str(two_class_example)
```

Make predictions on the test data; `.pred_1` is the "probability" of spam.

```{r pred_spam_test}
predict(rf$fit, spam_test, type = 'prob')
```

Predict and generate table in the format of `two_class_example` using a wrapper function.

* `fit` - model
* `test_data` - test data
* `pos` - class that the model is testing for
* `neg` - the other class

Since the model is testing for spam, `pos` is 'spam'.

```{r predict}
predict_wrapper <- function(fit, test_data, pos, neg, type = 'prob'){
  predict(fit, test_data, type = type) |>
    mutate(truth = ifelse(as.integer(test_data$class) == 2, pos, neg)) |>
    mutate(truth = factor(truth, levels = c(pos, neg))) |>
    rename(
      ham = .pred_0,
      spam = .pred_1
    ) |>
    mutate(
      predicted = ifelse(spam > 0.5, pos, neg)
    ) |>
    mutate(
      predicted = factor(predicted, levels = c(pos, neg))
    ) |>
    select(truth, everything())
}

rf$predictions <- predict_wrapper(rf$fit, spam_test, 'spam', 'ham')
rf$predictions
```

Confusion matrix.

```{r confusion_matrix}
cm <- table(rf$predictions$truth, rf$predictions$predicted)
cm |>
  prop.table()
```

Metrics.

```{r calculate_metrics}
metrics(rf$predictions, truth, predicted)
```

[table_metrics](https://github.com/davetang/learning_r/blob/main/code/table_metrics.R).

```{r table_metrics}
source("https://raw.githubusercontent.com/davetang/learning_r/main/code/table_metrics.R")
table_metrics(cm, 'spam', 'ham', 'row', sig_fig = 7)
```

Area under the PR curve.

```{r pr_auc}
pr_auc(rf$predictions, truth, spam)
```

[PR curve](https://yardstick.tidymodels.org/reference/pr_curve.html).

```{r pr_curve}
pr_curve(rf$predictions, truth, spam) |>
  ggplot(aes(x = recall, y = precision)) +
  geom_path() +
  coord_equal() +
  ylim(c(0, 1)) +
  ggtitle('PR curve')
```

Area under the ROC curve.

```{r roc_auc}
roc_auc(rf$predictions, truth, spam)
```

[ROC curve](https://yardstick.tidymodels.org/reference/roc_curve.html).

```{r roc_curve}
roc_curve(rf$predictions, truth, spam) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  ggtitle('ROC curve')
```

### Using ROCR

Compare with [ROCR](https://cran.rstudio.com/web/packages/ROCR/vignettes/ROCR.html).

Every classifier evaluation using {ROCR} starts with creating a prediction object.

```{r rocr_pred}
predictions <- predict(rf$fit, spam_test, type = 'prob')$.pred_1
labels <- spam_test$class
pred <- prediction(predictions, labels)
aucpr <- performance(pred, "aucpr")
aucroc <- performance(pred, "auc")
str(aucpr)
str(aucroc)
```

PR curve.

```{r rocr_pr_curve}
perf <- performance(pred, "prec", "rec")
plot(perf, lwd= 1, main= "PR curve")
```

ROC curve.

```{r rocr_roc_curve}
perf <- performance(pred, "tpr", "fpr")
plot(perf, lwd= 1, main= "ROC curve")
```

### Class imbalance

Difference in area under the ROC curve and area under the precision recall curve.

```{r class_imbalance}
set.seed(1984)

n <- 1000
p_positive <- 0.05

y <- factor(rbinom(n, 1, p_positive))
x <- rnorm(n, mean = ifelse(y == 1, 2, 0), sd = 1)

fit <- glm(y ~ x, family = binomial)
probs <- predict(fit, type = "response")

data.frame(
  truth = ifelse(y == 1, 'DA', 'NotDA'),
  NotDA = 1 - probs,
  DA = probs
) |>
  dplyr::mutate(truth = factor(truth, levels = c('DA', 'NotDA'))) -> toy_data
```

Identify most `DA` (>96%) but a lot of `NotDA` (>43%) are also predicted as `DA`, i.e., false positives.

```{r class_imbalance_table}
toy_data |>
  dplyr::mutate(
    predicted = factor(ifelse(DA > 0.01, 'DA', 'NotDA'), levels = c('DA', 'NotDA'))
  ) |>
  dplyr::select(truth, predicted) |>
  table() |>
  prop.table(margin = 1)
```

Area under the ROC curve (high if we can rank positives higher than negatives)

```{r imbalance_roc_curve}
roc_curve(toy_data, truth, DA) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  ggtitle(round(roc_auc(toy_data, truth, DA)$.estimate, 5)) +
  theme_minimal()
```

Area under the precision recall curve (sensitive to false positives)

```{r imbalance_pr_curve}
pr_curve(toy_data, truth, DA) |>
  ggplot(aes(x = recall, y = precision)) +
  geom_path() +
  coord_equal() +
  ylim(c(0, 1)) +
  ggtitle(round(pr_auc(toy_data, truth, DA)$.estimate, 5)) +
  theme_minimal()
```

## Session info

Time built.

```{r time, echo=FALSE}
Sys.time()
```

Session info.

```{r session_info, echo=FALSE}
sessionInfo()
```
